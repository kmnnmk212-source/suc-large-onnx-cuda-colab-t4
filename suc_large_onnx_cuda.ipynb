{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TZXfTU007MD",
        "outputId": "f659082d-896e-46f6-a9f0-1df73ed26923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-genai-cuda\n",
            "  Downloading onnxruntime_genai_cuda-0.11.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (676 bytes)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime-genai-cuda) (2.0.2)\n",
            "Collecting onnxruntime-gpu>=1.23.2 (from onnxruntime-genai-cuda)\n",
            "  Downloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu>=1.23.2->onnxruntime-genai-cuda) (1.3.0)\n",
            "Downloading onnxruntime_genai_cuda-0.11.2-cp312-cp312-manylinux_2_28_x86_64.whl (88.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (300.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu, onnxruntime-genai-cuda\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-genai-cuda-0.11.2 onnxruntime-gpu-1.23.2\n"
          ]
        }
      ],
      "source": [
        "!pip install onnxruntime-genai-cuda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download Prince-1/Granite-3.3-2B-Instruct-Onnx --local-dir ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDaqxF_T1Bmp",
        "outputId": "92c9c9f4-652f-483f-d129-9ddd35b45755"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Fetching 11 files:   0% 0/11 [00:00<?, ?it/s]Downloading 'model.onnx' to '.cache/huggingface/download/ihhw_uFzBe-Y54_HOJQmXx4GS8A=.3addb7b50e098eba839394cdf9ead1b4535b50dd9b62e44ce768d4c251761ccf.incomplete'\n",
            "Downloading 'model.onnx.data' to '.cache/huggingface/download/Lzxw0O44RMmhTgKWeQQ-MMzHFnE=.f05e71817056a9977ec86d8df863c745aac47f1bcd23257f1648fc03e4bac5e6.incomplete'\n",
            "Downloading 'special_tokens_map.json' to '.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.4ccaca4ecb4010d47211e47ab421192ad7923c30.incomplete'\n",
            "Downloading 'genai_config.json' to '.cache/huggingface/download/yDyKFBRCOr3XTWAm4EHNQUX0s3Y=.a05f58790b190718f330e810115de585c5642319.incomplete'\n",
            "Downloading 'merges.txt' to '.cache/huggingface/download/PtHk0z_I45atnj23IIRhTExwT3w=.f8479fb696fe07332c55300a6accf8cc191acc6a.incomplete'\n",
            "Downloading 'added_tokens.json' to '.cache/huggingface/download/SeqzFlf9ZNZ3or_wZAOIdsM3Yxw=.944f7a55b05d9c0ce245208d27cb11b67b986d6b.incomplete'\n",
            "Downloading '.gitattributes' to '.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.705765cea3520406fae64d16b6da31b72948a9a0.incomplete'\n",
            "\n",
            "model.onnx.data:   0% 0.00/5.29G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model.onnx:   0% 0.00/951k [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'README.md' to '.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.ea76a79773464ce6e4bf4f9640cdf18b6e955d78.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "added_tokens.json: 100% 207/207 [00:00<00:00, 1.81MB/s]\n",
            "Download complete. Moving file to added_tokens.json\n",
            "\n",
            "\n",
            "\n",
            "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 1.57kB [00:00, 5.66MB/s]\n",
            "Download complete. Moving file to .gitattributes\n",
            "Fetching 11 files:   9% 1/11 [00:00<00:03,  2.81it/s]\n",
            "\n",
            "\n",
            "\n",
            "README.md: 27.6kB [00:00, 86.4MB/s]\n",
            "Download complete. Moving file to README.md\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "special_tokens_map.json: 100% 801/801 [00:00<00:00, 11.1MB/s]\n",
            "Download complete. Moving file to special_tokens_map.json\n",
            "merges.txt: 442kB [00:00, 6.80MB/s]\n",
            "Download complete. Moving file to merges.txt\n",
            "\n",
            "\n",
            "\n",
            "genai_config.json: 1.51kB [00:00, 5.46MB/s]\n",
            "Download complete. Moving file to genai_config.json\n",
            "Downloading 'tokenizer_config.json' to '.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.c3dadc5c761f76643f4226f1b93012ea9d7f9245.incomplete'\n",
            "Downloading 'vocab.json' to '.cache/huggingface/download/j3m-Hy6QvBddw8RXA1uSWl1AJ0c=.0a11f2016e660fd490f7bf168e6d1f9c86a8f744.incomplete'\n",
            "Downloading 'tokenizer.json' to '.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.4dbede017f0aa7bd21a51925fefd87fe788eb6d1.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "vocab.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 9.93kB [00:00, 41.7MB/s]\n",
            "Download complete. Moving file to tokenizer_config.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vocab.json: 121kB [00:00, 1.09MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "vocab.json: 777kB [00:00, 6.19MB/s]\n",
            "Download complete. Moving file to vocab.json\n",
            "tokenizer.json: 3.48MB [00:00, 21.1MB/s]\n",
            "Download complete. Moving file to tokenizer.json\n",
            "\n",
            "\n",
            "model.onnx: 100% 951k/951k [00:01<00:00, 796kB/s]\n",
            "Download complete. Moving file to model.onnx\n",
            "Fetching 11 files:  55% 6/11 [00:01<00:01,  4.22it/s]\n",
            "model.onnx.data:   0% 1.85M/5.29G [00:02<1:58:35, 742kB/s]\u001b[A\n",
            "model.onnx.data:   1% 53.8M/5.29G [00:03<04:41, 18.6MB/s] \u001b[A\n",
            "model.onnx.data:   2% 121M/5.29G [00:04<02:06, 41.0MB/s] \u001b[A\n",
            "model.onnx.data:   4% 188M/5.29G [00:04<01:34, 53.8MB/s]\u001b[A\n",
            "model.onnx.data:   9% 496M/5.29G [00:07<00:47, 101MB/s] \u001b[A\n",
            "model.onnx.data:  12% 631M/5.29G [00:07<00:34, 135MB/s]\u001b[A\n",
            "model.onnx.data:  13% 698M/5.29G [00:07<00:29, 153MB/s]\u001b[A\n",
            "model.onnx.data:  14% 739M/5.29G [00:11<01:21, 55.6MB/s]\u001b[A\n",
            "model.onnx.data:  17% 873M/5.29G [00:11<00:51, 85.1MB/s]\u001b[A\n",
            "model.onnx.data:  18% 940M/5.29G [00:11<00:41, 104MB/s] \u001b[A\n",
            "model.onnx.data:  19% 1.01G/5.29G [00:11<00:33, 129MB/s]\u001b[A\n",
            "model.onnx.data:  20% 1.07G/5.29G [00:12<00:29, 143MB/s]\u001b[A\n",
            "model.onnx.data:  22% 1.17G/5.29G [00:12<00:22, 184MB/s]\u001b[A\n",
            "model.onnx.data:  23% 1.24G/5.29G [00:12<00:20, 199MB/s]\u001b[A\n",
            "model.onnx.data:  24% 1.28G/5.29G [00:12<00:20, 200MB/s]\u001b[A\n",
            "model.onnx.data:  25% 1.34G/5.29G [00:13<00:18, 219MB/s]\u001b[A\n",
            "model.onnx.data:  27% 1.41G/5.29G [00:15<00:52, 74.4MB/s]\u001b[A\n",
            "model.onnx.data:  28% 1.48G/5.29G [00:15<00:44, 85.0MB/s]\u001b[A\n",
            "model.onnx.data:  32% 1.71G/5.29G [00:16<00:19, 183MB/s] \u001b[A\n",
            "model.onnx.data:  34% 1.77G/5.29G [00:16<00:17, 203MB/s]\u001b[A\n",
            "model.onnx.data:  35% 1.84G/5.29G [00:16<00:15, 220MB/s]\u001b[A\n",
            "model.onnx.data:  36% 1.91G/5.29G [00:16<00:16, 209MB/s]\u001b[A\n",
            "model.onnx.data:  37% 1.97G/5.29G [00:17<00:13, 243MB/s]\u001b[A\n",
            "model.onnx.data:  39% 2.04G/5.29G [00:17<00:13, 245MB/s]\u001b[A\n",
            "model.onnx.data:  40% 2.11G/5.29G [00:19<00:37, 84.8MB/s]\u001b[A\n",
            "model.onnx.data:  42% 2.24G/5.29G [00:19<00:22, 134MB/s] \u001b[A\n",
            "model.onnx.data:  44% 2.31G/5.29G [00:23<00:56, 52.7MB/s]\u001b[A\n",
            "model.onnx.data:  45% 2.38G/5.29G [00:23<00:42, 68.0MB/s]\u001b[A\n",
            "model.onnx.data:  48% 2.51G/5.29G [00:23<00:26, 104MB/s] \u001b[A\n",
            "model.onnx.data:  49% 2.58G/5.29G [00:24<00:22, 120MB/s]\u001b[A\n",
            "model.onnx.data:  50% 2.65G/5.29G [00:24<00:18, 142MB/s]\u001b[A\n",
            "model.onnx.data:  51% 2.71G/5.29G [00:24<00:15, 161MB/s]\u001b[A\n",
            "model.onnx.data:  53% 2.78G/5.29G [00:24<00:14, 179MB/s]\u001b[A\n",
            "model.onnx.data:  54% 2.85G/5.29G [00:25<00:12, 201MB/s]\u001b[A\n",
            "model.onnx.data:  55% 2.91G/5.29G [00:25<00:11, 206MB/s]\u001b[A\n",
            "model.onnx.data:  56% 2.98G/5.29G [00:25<00:10, 222MB/s]\u001b[A\n",
            "model.onnx.data:  58% 3.05G/5.29G [00:26<00:09, 232MB/s]\u001b[A\n",
            "model.onnx.data:  58% 3.09G/5.29G [00:26<00:08, 250MB/s]\u001b[A\n",
            "model.onnx.data:  60% 3.17G/5.29G [00:26<00:09, 224MB/s]\u001b[A\n",
            "model.onnx.data:  61% 3.23G/5.29G [00:26<00:08, 252MB/s]\u001b[A\n",
            "model.onnx.data:  62% 3.30G/5.29G [00:27<00:08, 239MB/s]\u001b[A\n",
            "model.onnx.data:  64% 3.37G/5.29G [00:27<00:07, 243MB/s]\u001b[A\n",
            "model.onnx.data:  65% 3.44G/5.29G [00:27<00:07, 252MB/s]\u001b[A\n",
            "model.onnx.data:  66% 3.50G/5.29G [00:27<00:07, 240MB/s]\u001b[A\n",
            "model.onnx.data:  68% 3.57G/5.29G [00:28<00:06, 249MB/s]\u001b[A\n",
            "model.onnx.data:  68% 3.61G/5.29G [00:28<00:06, 262MB/s]\u001b[A\n",
            "model.onnx.data:  70% 3.68G/5.29G [00:28<00:09, 163MB/s]\u001b[A\n",
            "model.onnx.data:  71% 3.74G/5.29G [00:29<00:07, 197MB/s]\u001b[A\n",
            "model.onnx.data:  72% 3.81G/5.29G [00:29<00:06, 228MB/s]\u001b[A\n",
            "model.onnx.data:  73% 3.88G/5.29G [00:29<00:06, 225MB/s]\u001b[A\n",
            "model.onnx.data:  75% 3.94G/5.29G [00:29<00:05, 224MB/s]\u001b[A\n",
            "model.onnx.data:  76% 4.01G/5.29G [00:30<00:05, 229MB/s]\u001b[A\n",
            "model.onnx.data:  77% 4.08G/5.29G [00:30<00:04, 243MB/s]\u001b[A\n",
            "model.onnx.data:  78% 4.15G/5.29G [00:30<00:04, 255MB/s]\u001b[A\n",
            "model.onnx.data:  80% 4.21G/5.29G [00:30<00:04, 249MB/s]\u001b[A\n",
            "model.onnx.data:  81% 4.28G/5.29G [00:31<00:03, 256MB/s]\u001b[A\n",
            "model.onnx.data:  82% 4.35G/5.29G [00:31<00:03, 250MB/s]\u001b[A\n",
            "model.onnx.data:  84% 4.41G/5.29G [00:31<00:03, 254MB/s]\u001b[A\n",
            "model.onnx.data:  85% 4.48G/5.29G [00:32<00:03, 259MB/s]\u001b[A\n",
            "model.onnx.data:  86% 4.55G/5.29G [00:32<00:02, 272MB/s]\u001b[A\n",
            "model.onnx.data:  87% 4.61G/5.29G [00:32<00:02, 274MB/s]\u001b[A\n",
            "model.onnx.data:  89% 4.68G/5.29G [00:32<00:02, 235MB/s]\u001b[A\n",
            "model.onnx.data:  90% 4.75G/5.29G [00:33<00:02, 267MB/s]\u001b[A\n",
            "model.onnx.data:  91% 4.82G/5.29G [00:33<00:01, 300MB/s]\u001b[A\n",
            "model.onnx.data:  92% 4.88G/5.29G [00:33<00:01, 277MB/s]\u001b[A\n",
            "model.onnx.data:  94% 4.95G/5.29G [00:33<00:01, 284MB/s]\u001b[A\n",
            "model.onnx.data:  95% 5.02G/5.29G [00:35<00:03, 82.0MB/s]\u001b[A\n",
            "model.onnx.data:  97% 5.15G/5.29G [00:36<00:00, 139MB/s] \u001b[A\n",
            "model.onnx.data:  99% 5.22G/5.29G [00:37<00:00, 96.6MB/s]\u001b[A\n",
            "model.onnx.data: 100% 5.29G/5.29G [00:37<00:00, 141MB/s]\n",
            "Download complete. Moving file to model.onnx.data\n",
            "Fetching 11 files: 100% 11/11 [00:37<00:00,  3.44s/it]\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zSRWu4jY2fnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/onnxruntime-genai.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBsGryrY1Zh3",
        "outputId": "2bddd381-2ebc-4982-ba2c-e2f2c9964c60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'onnxruntime-genai'...\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/microsoft/onnxruntime-genai/archive/refs/tags/v0.11.2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0au9eGuz2zC9",
        "outputId": "2bf35874-e576-4d1d-fb4f-4db84695dba3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-18 21:26:48--  https://github.com/microsoft/onnxruntime-genai/archive/refs/tags/v0.11.2.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/microsoft/onnxruntime-genai/zip/refs/tags/v0.11.2 [following]\n",
            "--2025-11-18 21:26:48--  https://codeload.github.com/microsoft/onnxruntime-genai/zip/refs/tags/v0.11.2\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.114.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.114.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘v0.11.2.zip’\n",
            "\n",
            "v0.11.2.zip             [      <=>           ]   9.04M  8.17MB/s    in 1.1s    \n",
            "\n",
            "2025-11-18 21:26:49 (8.17 MB/s) - ‘v0.11.2.zip’ saved [9480647]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip v0.11.2.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5TkRom13Ris",
        "outputId": "9384d7d7-9b8a-4d11-d8cf-013ee59f3148"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  v0.11.2.zip\n",
            "25962b07e19d91fdae570d776228b7425325e433\n",
            "   creating: onnxruntime-genai-0.11.2/\n",
            "  inflating: onnxruntime-genai-0.11.2/.clang-format  \n",
            "   creating: onnxruntime-genai-0.11.2/.config/\n",
            "   creating: onnxruntime-genai-0.11.2/.config/1espt/\n",
            "  inflating: onnxruntime-genai-0.11.2/.config/1espt/PipelineAutobaseliningConfig.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.config/guardian/\n",
            "  inflating: onnxruntime-genai-0.11.2/.config/guardian/.gdnbaselines  \n",
            "  inflating: onnxruntime-genai-0.11.2/.config/tsaoptions.json  \n",
            "   creating: onnxruntime-genai-0.11.2/.github/\n",
            "   creating: onnxruntime-genai-0.11.2/.github/ISSUE_TEMPLATE/\n",
            "  inflating: onnxruntime-genai-0.11.2/.github/ISSUE_TEMPLATE/bug_report.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/copilot-instructions.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/labeler.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.github/policies/\n",
            "  inflating: onnxruntime-genai-0.11.2/.github/policies/issueLabeler.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/policies/test_issueLabeler.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.github/workflows/\n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/android-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/clang-format-lint.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/codeql.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/ios-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/linux-cpu-arm64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/linux-cpu-x64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/linux-cpu-x64-nightly-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/linux-gpu-x64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/mac-cpu-arm64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/run-issue-labeler.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/win-cpu-arm64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/win-cpu-x64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/win-cuda-x64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/win-directml-x64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.github/workflows/win-winml-x64-build.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.gitignore  \n",
            " extracting: onnxruntime-genai-0.11.2/.gitmodules  \n",
            "   creating: onnxruntime-genai-0.11.2/.pipelines/\n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/android-publishing.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/codeql.yaml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/macos-ios-cocoapods-publishing.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/nuget-publishing.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/policheck_exclusions.xml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/pypl-publishing.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.pipelines/stages/\n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/android-packaging-stage.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/capi-packaging-stage.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/custom-nuget-packaging-stage.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/\n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/android-java-api-aar.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/capi-packaging-job.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/custom-nuget-packaging-job.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/macos-ios-cocoapods-packaging-job.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/nuget-packaging-job.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/nuget-validation-job.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/py-packaging-job.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/py-validation-job.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/\n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/capi-appleframework-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/capi-linux-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/capi-macos-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/capi-win-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/compliant-and-cleanup-step.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/compliant/\n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/compliant/component-governance-component-detection-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/compliant/esrp_nuget.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/compliant/mac-esrp-archive-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/compliant/win-esrp-dll-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/nuget-validation-step.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/python-validation-step.yml  \n",
            "   creating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/\n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/capi-archive.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/download-huggingface-model.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/flex-download-pipeline-artifact.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/get-nuget-package-version-as-variable.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/set-cmake-build-type.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/set-genai-version.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/set-nightly-build-option-variable.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/steps/utils/use-android-ndk.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/jobs/winml-nuget-packaging-job.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/macos-ios-cocoapods-packaging-stage.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/nuget-packaging-stage.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/nuget-validation-stage.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/py-packaging-stage.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/.pipelines/stages/py-validation-stage.yml  \n",
            "  inflating: onnxruntime-genai-0.11.2/CMakeLists.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/CMakePresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/CODE_OF_CONDUCT.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/LICENSE  \n",
            "  inflating: onnxruntime-genai-0.11.2/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/SECURITY.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/ThirdPartyNotices.txt  \n",
            " extracting: onnxruntime-genai-0.11.2/VERSION_INFO  \n",
            "   creating: onnxruntime-genai-0.11.2/benchmark/\n",
            "   creating: onnxruntime-genai-0.11.2/benchmark/c/\n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/CMakeLists.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/main.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/options.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/options.h  \n",
            "   creating: onnxruntime-genai-0.11.2/benchmark/c/posix/\n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/posix/resource_utils.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/readme.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/resource_utils.h  \n",
            "   creating: onnxruntime-genai-0.11.2/benchmark/c/windows/\n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/c/windows/resource_utils.cpp  \n",
            "   creating: onnxruntime-genai-0.11.2/benchmark/python/\n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/python/README  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/python/benchmark_e2e.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/python/benchmark_e2e_continuous_test.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/python/benchmark_multimodal.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/python/metrics.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/benchmark/python/prompts.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/build.bat  \n",
            "  inflating: onnxruntime-genai-0.11.2/build.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/build.sh  \n",
            "   creating: onnxruntime-genai-0.11.2/cgmanifests/\n",
            "  inflating: onnxruntime-genai-0.11.2/cgmanifests/generate_cgmanifest.py  \n",
            "   creating: onnxruntime-genai-0.11.2/cgmanifests/generated/\n",
            "  inflating: onnxruntime-genai-0.11.2/cgmanifests/generated/cgmanifest.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cgmanifests/print_submodule_info.py  \n",
            "   creating: onnxruntime-genai-0.11.2/cmake/\n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/Info.plist.in  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/check_cuda.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/check_dml.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/check_rocm.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/cxx_standard.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/deps.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/\n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/helper_functions.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/onnxruntime_external_deps.cmake  \n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/opencv/\n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/COPYRIGHT  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/LICENSE  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/README.md  \n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/\n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/\n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/\n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Modules/\n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Modules/Platform/\n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Modules/Platform/iOS.cmake  \n",
            "   creating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Toolchains/\n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Toolchains/Toolchain-Catalyst_Xcode.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Toolchains/Toolchain-iPhoneOS_Xcode.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Toolchains/Toolchain-iPhoneSimulator_Xcode.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Toolchains/common-ios-toolchain.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/external/opencv/platforms/ios/cmake/Toolchains/xcodebuild_wrapper.in  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/global_variables.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/nuget.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/options.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/ortlib.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/package.cmake  \n",
            "   creating: onnxruntime-genai-0.11.2/cmake/presets/\n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeLinuxBuildPresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeLinuxClangConfigPresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeLinuxDefaultConfigPresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeLinuxGccConfigPresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeMacOSBuildPresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeMacOSConfigPresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeWinBuildPresets.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/cmake/presets/CMakeWinConfigPresets.json  \n",
            "   creating: onnxruntime-genai-0.11.2/documents/\n",
            "  inflating: onnxruntime-genai-0.11.2/documents/DownloadModels.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/documents/Runtime_option.md  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/\n",
            "   creating: onnxruntime-genai-0.11.2/examples/c/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/.gitignore  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/CMakeLists.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/README.md  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/c/include/\n",
            " extracting: onnxruntime-genai-0.11.2/examples/c/include/.gitkeep  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/c/lib/\n",
            " extracting: onnxruntime-genai-0.11.2/examples/c/lib/.gitkeep  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/c/src/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/src/common.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/src/common.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/src/model_chat.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/src/model_qa.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/src/model_vision.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/src/phi4-mm.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/c/src/whisper.cpp  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/chat_app/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/__init__.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/app.py  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/chat_app/app_modules/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/app_modules/overwrites.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/app_modules/presets.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/app_modules/utils.py  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/chat_app/assets/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/assets/custom.css  \n",
            " extracting: onnxruntime-genai-0.11.2/examples/chat_app/assets/custom.js  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/consts.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/image.png  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/chat_app/interface/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/interface/hddr_llm_onnx_interface.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/interface/multimodal_onnx_interface.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/chat_app/vision_UI_interface.png  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/\n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/.gitignore  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Assets/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Assets/Screenshot1.PNG  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Assets/Screenshot2.PNG  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny.sln  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/App.xaml  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/App.xaml.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/AssemblyInfo.cs  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Controls/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Controls/SearchOptionsControl.xaml  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Controls/SearchOptionsControl.xaml.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Extensions.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Genny.csproj  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Images/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Images/robot.png  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Images/user.png  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/MainWindow.xaml  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/MainWindow.xaml.cs  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Utils/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Utils/AutoScrollBehavior.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Utils/RelayCommand.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Utils/ShiftEnterBehavior.cs  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/ViewModel/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/ViewModel/ConfigurationModel.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/ViewModel/ModelOptionsModel.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/ViewModel/ResultModel.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/ViewModel/SearchOptionsModel.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/ViewModel/TokenModel.cs  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Views/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Views/StatefulView.xaml  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Views/StatefulView.xaml.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Views/StatelessView.xaml  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Views/StatelessView.xaml.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Views/TokenizerView.xaml  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/Genny/Views/TokenizerView.xaml.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/Genny/README.md  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi/HelloPhi.csproj  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi/HelloPhi.sln  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi/Program.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi/README.md  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi3V/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi3V/HelloPhi3V.csproj  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi3V/HelloPhi3V.sln  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi3V/Program.cs  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi4MM/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi4MM/HelloPhi4MM.csproj  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi4MM/HelloPhi4MM.sln  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/csharp/HelloPhi4MM/Program.cs  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/python/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/awq-quantized-model.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/awq-quantized-model.py  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/python/engine/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/engine/continuous-batching.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/engine/model-qa.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/generate-e2e-example.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/guidance-example.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/model-chat.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/model-generate.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/model-qa.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/model-vision.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/phi-3-tutorial.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/phi-3-vision.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/phi-3.5-vision.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/phi-4-multi-modal.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/phi3-qa.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/phi4-mm.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/qa-e2e-example.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/python/whisper.py  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/slm_engine/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/architecture.drawio.png  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/\n",
            " extracting: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/.gitignore  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/Dockerfile  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/build.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/build_android.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/build_deps.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/build_linux.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/build_scripts/docker_shell.sh  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/slm_engine/src/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/CMakeLists.txt  \n",
            " extracting: onnxruntime-genai-0.11.2/examples/slm_engine/src/VERSION.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/CMakeLists.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/gtest_main.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/input_decoder.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/input_decoder.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/input_decoder_test.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/slm_engine.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/slm_engine.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/slm_engine_test.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/slm_runner.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/src/cpp/slm_server.cpp  \n",
            "   creating: onnxruntime-genai-0.11.2/examples/slm_engine/test/\n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/10-inputs-ground-truth.jsonl  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/10-inputs-to-slm.jsonl  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/README_TOOL_CALLING.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/batch-input.jsonl  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/chat_ui.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/sample-qNa-data.jsonl  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/test-slm-server-tools.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/test-slm-server.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/test_slm_server.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/examples/slm_engine/test/test_tool_calling.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/generate_dml_shaders.bat  \n",
            "  inflating: onnxruntime-genai-0.11.2/nuget.config  \n",
            "   creating: onnxruntime-genai-0.11.2/nuget/\n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/MANAGED_PACKAGE.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/Microsoft.ML.OnnxRuntimeGenAI.Managed.nuspec  \n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/PACKAGE.md  \n",
            "   creating: onnxruntime-genai-0.11.2/nuget/targets/\n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/targets/Microsoft.ML.OnnxRuntimeGenAI.Managed.targets  \n",
            "   creating: onnxruntime-genai-0.11.2/nuget/targets/net9.0-android/\n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/targets/net9.0-android/Microsoft.ML.OnnxRuntimeGenAI.targets  \n",
            "   creating: onnxruntime-genai-0.11.2/nuget/targets/net9.0-ios/\n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/targets/net9.0-ios/Microsoft.ML.OnnxRuntimeGenAI.targets  \n",
            "   creating: onnxruntime-genai-0.11.2/nuget/targets/net9.0-maccatalyst/\n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/targets/net9.0-maccatalyst/README.md  \n",
            " extracting: onnxruntime-genai-0.11.2/nuget/targets/net9.0-maccatalyst/_._  \n",
            "   creating: onnxruntime-genai-0.11.2/nuget/targets/netstandard/\n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/targets/netstandard/Microsoft.ML.OnnxRuntimeGenAI.props  \n",
            "  inflating: onnxruntime-genai-0.11.2/nuget/targets/netstandard/Microsoft.ML.OnnxRuntimeGenAI.targets  \n",
            "  inflating: onnxruntime-genai-0.11.2/onnxruntime-genai.sln  \n",
            "  inflating: onnxruntime-genai-0.11.2/packages.config  \n",
            "   creating: onnxruntime-genai-0.11.2/src/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/.clang-tidy  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/beam_search_scorer.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/beam_search_scorer.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/beam_search_topk.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/config.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/config.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/constrained_logits_processor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/constrained_logits_processor.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/cpu/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/cpu/interface.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cpu/interface.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/csharp/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Adapters.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Audios.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Config.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Exceptions.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Generator.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/GeneratorParams.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Images.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Microsoft.ML.OnnxRuntimeGenAI.csproj  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Microsoft.ML.OnnxRuntimeGenAI.sln  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Model.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/MultiModalProcessor.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/NativeMethods.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/OnnxRuntimeGenAIChatClient.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/OnnxRuntimeGenAIChatClientOptions.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Result.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Sequences.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Tensor.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Tokenizer.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/TokenizerStream.cs  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/csharp/Utils.cs  \n",
            " extracting: onnxruntime-genai-0.11.2/src/csharp/onnxruntime-genai.snk  \n",
            "   creating: onnxruntime-genai-0.11.2/src/cuda/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/beam_search_scorer_cuda.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/beam_search_scorer_cuda.cu  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/beam_search_scorer_cuda.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/beam_search_scorer_cuda.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/beam_search_topk.cu  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_common.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_sampling.cu  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_sampling.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk.cu  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_benchmark.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_benchmark_cache.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_cascaded_sort.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_common.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_distributed_select_sort.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_flash_convergent.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_full_sort.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_hybrid_sort.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_iterative_sort.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_per_batch_radix_sort.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_select_sort.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_softmax.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_sort_benchmark_cache.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_stable_sort_helper.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/cuda_topk_warp_sort_helper.cuh  \n",
            " extracting: onnxruntime-genai-0.11.2/src/cuda/exported_symbols.lst  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/interface.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/interface.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/kernels.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/model_kernels.cu  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/search_cuda.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/search_cuda.cu  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/search_cuda.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/search_cuda.h  \n",
            " extracting: onnxruntime-genai-0.11.2/src/cuda/symbols.def  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/cuda/version_script.lds  \n",
            "   creating: onnxruntime-genai-0.11.2/src/dll/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/dll/onnxruntime-genai.rc  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dll_load_error.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dll_load_error.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/dml/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_adapter_info.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_adapter_info.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_adapter_selection.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_adapter_selection.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_command_allocator_ring.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_command_queue.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_command_queue.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_command_recorder.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_command_recorder.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_descriptor_pool.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_descriptor_pool.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_execution_context.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_execution_context.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_gpu_event.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_helpers.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_helpers.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_increment_values_kernel.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_increment_values_kernel.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_pooled_upload_heap.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_pooled_upload_heap.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_readback_heap.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_readback_heap.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/dml/dml_shaders/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_shaders/dml_increment_values.hlsl  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_shaders/dml_update_attention_mask.hlsl  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_update_mask_kernel.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/dml_update_mask_kernel.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/dml/generated_dml_shaders/\n",
            " extracting: onnxruntime-genai-0.11.2/src/dml/generated_dml_shaders/.clang-format  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/generated_dml_shaders/increment_values_int32.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/generated_dml_shaders/increment_values_int64.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/generated_dml_shaders/update_mask_int32.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/generated_dml_shaders/update_mask_int64.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/interface.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/dml/interface.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/engine/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/block.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/block.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/cache_manager.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/cache_manager.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/engine/decoders/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/decoders/decoder.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/decoders/simple_decoder.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/decoders/simple_decoder.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/decoders/static_batch_decoder_io.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/decoders/static_batch_decoder_io.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/decoders/varlen_decoder_io.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/decoders/varlen_decoder_io.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/engine.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/engine.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/model_executor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/model_executor.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/model_io.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/paged_key_value_cache.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/paged_key_value_cache.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/request.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/request.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/scheduled_requests.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/scheduled_requests.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/scheduler.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/engine/scheduler.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/filesystem.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/generators.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/generators.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/AndroidBuild.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/CMakeLists.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/Debugging.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/UpdatingJavaBindings.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/build-android.gradle  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/build.gradle  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/gradle/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/gradle/wrapper/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/gradle/wrapper/gradle-wrapper.jar  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/gradle/wrapper/gradle-wrapper.properties  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/gradlew  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/gradlew.bat  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/settings-android.gradle  \n",
            " extracting: onnxruntime-genai-0.11.2/src/java/settings.gradle  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/main/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/AndroidManifest.xml  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/main/java/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Adapters.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Audios.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Config.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/GenAI.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/GenAIException.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Generator.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/GeneratorParams.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Images.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Model.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/MultiModalProcessor.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/NamedTensors.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Sequences.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/SimpleGenAI.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Tensor.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/Tokenizer.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/TokenizerStream.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/java/ai/onnxruntime/genai/package-info.java  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/main/native/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Adapters.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Audios.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Config.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_GenAI.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Generator.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_GeneratorParams.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Images.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Model.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_MultiModalProcessor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_NamedTensors.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Sequences.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Tensor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_Tokenizer.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/ai_onnxruntime_genai_TokenizerStream.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/utils.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/main/native/utils.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/.gitignore  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/README.md  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/build.gradle  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/proguard-rules.pro  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/androidTest/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/androidTest/java/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/androidTest/java/ai/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/androidTest/java/ai/onnxruntime_genai/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/androidTest/java/ai/onnxruntime_genai/example/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/androidTest/java/ai/onnxruntime_genai/example/javavalidator/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/androidTest/java/ai/onnxruntime_genai/example/javavalidator/SimpleTest.kt  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/AndroidManifest.xml  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/java/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/java/ai/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/java/ai/onnxruntime_genai/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/java/ai/onnxruntime_genai/example/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/java/ai/onnxruntime_genai/example/javavalidator/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/java/ai/onnxruntime_genai/example/javavalidator/MainActivity.kt  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/drawable-v24/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/drawable-v24/ic_launcher_foreground.xml  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/drawable/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/drawable/ic_launcher_background.xml  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/layout/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/layout/activity_main.xml  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-anydpi-v26/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-anydpi-v26/ic_launcher.xml  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-anydpi-v26/ic_launcher_round.xml  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-hdpi/\n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-hdpi/ic_launcher.png  \n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-hdpi/ic_launcher_round.png  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-mdpi/\n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-mdpi/ic_launcher.png  \n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-mdpi/ic_launcher_round.png  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xhdpi/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xhdpi/ic_launcher.png  \n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xhdpi/ic_launcher_round.png  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xxhdpi/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xxhdpi/ic_launcher.png  \n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.png  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xxxhdpi/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xxxhdpi/ic_launcher.png  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.png  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/values-night/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/values-night/themes.xml  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/values/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/values/colors.xml  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/values/strings.xml  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/app/src/main/res/values/themes.xml  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/build.gradle  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/android/gradle.properties  \n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/android/settings.gradle  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/java/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/GenAITestExecutionListener.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/GenerationTest.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/GeneratorParamsTest.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/MultiModalProcessorTest.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/TensorTest.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/TestUtils.java  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/src/test/java/ai/onnxruntime/genai/TokenizerTest.java  \n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/resources/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/resources/META-INF/\n",
            "   creating: onnxruntime-genai-0.11.2/src/java/src/test/resources/META-INF/services/\n",
            " extracting: onnxruntime-genai-0.11.2/src/java/src/test/resources/META-INF/services/org.junit.platform.launcher.TestExecutionListener  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/java/windows-unittests.cmake  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/json.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/json.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/leakcheck.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/logging.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/logging.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/make_string.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/models/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/adapters.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/adapters.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/audio_features.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/audio_features.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/debugging.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/debugging.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/decoder_only.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/decoder_only.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/decoder_only_pipeline.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/decoder_only_pipeline.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/embeddings.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/embeddings.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/env_utils.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/env_utils.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/extra_inputs.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/extra_inputs.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/extra_outputs.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/extra_outputs.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/gemma_image_processor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/gemma_image_processor.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/gpt.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/gpt.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/input_ids.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/input_ids.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/kv_cache.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/kv_cache.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/logits.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/logits.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/marian.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/marian.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/model.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/model.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/model_type.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/multi_modal.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/multi_modal.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/multi_modal_features.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/multi_modal_features.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/onnxruntime_api.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/onnxruntime_inline.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/phi_image_processor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/phi_image_processor.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/phi_multimodal_processor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/phi_multimodal_processor.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/position_inputs.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/position_inputs.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/processor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/processor.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/threadpool.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/threadpool.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/utils.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/utils.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/whisper.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/whisper.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/whisper_processor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/whisper_processor.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/windowed_kv_cache.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/models/windowed_kv_cache.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/narrow.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/objectivec/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/cxx_api.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/error_utils.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/error_utils.mm  \n",
            "   creating: onnxruntime-genai-0.11.2/src/objectivec/include/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/include/ort_genai_objc.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_audios.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_generator.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_generator_params.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_images.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_internal.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_model.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_multi_modal_processor.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_named_tensors.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_sequences.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_tensor.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_tokenizer.mm  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/oga_tokenizer_stream.mm  \n",
            "   creating: onnxruntime-genai-0.11.2/src/objectivec/test/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/test/assertion_utils.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/objectivec/test/ort_genai_api_test.mm  \n",
            "   creating: onnxruntime-genai-0.11.2/src/openvino/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/openvino/interface.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/openvino/interface.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/ort_genai.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/ort_genai_c.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/ort_genai_c.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/python/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/CMakeLists.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/__init__.py.in  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/package_description.md  \n",
            "   creating: onnxruntime-genai-0.11.2/src/python/py/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/py/_dll_directory.py  \n",
            "   creating: onnxruntime-genai-0.11.2/src/python/py/models/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/py/models/DESIGN.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/py/models/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/py/models/builder.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/py/models/gguf_model.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/py/models/quantized_model.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/python.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/python/setup.py.in  \n",
            "   creating: onnxruntime-genai-0.11.2/src/qnn/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/qnn/interface.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/qnn/interface.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/runtime_settings.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/runtime_settings.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/search.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/search.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/sequences.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/sequences.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/smartptrs.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/softmax.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/softmax_cpu.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/span.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/tensor.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/tensor.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/tracing.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/tracing.h  \n",
            "   creating: onnxruntime-genai-0.11.2/src/webgpu/\n",
            "  inflating: onnxruntime-genai-0.11.2/src/webgpu/interface.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/webgpu/interface.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/src/worker_thread.h  \n",
            "   creating: onnxruntime-genai-0.11.2/test/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/CMakeLists.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/c_api_tests.cpp  \n",
            "   creating: onnxruntime-genai-0.11.2/test/csharp/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/csharp/Microsoft.ML.OnnxRuntimeGenAI.Tests.csproj  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/csharp/TestOnnxRuntimeGenAIAPI.cs  \n",
            "   creating: onnxruntime-genai-0.11.2/test/cuda_kernel/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/cuda_kernel/cuda_sampling_benchmark.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/cuda_kernel/cuda_sampling_tests.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/cuda_kernel/cuda_topk_benchmark.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/cuda_kernel/cuda_topk_tests.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/main.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/model_tests.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/narrow_test.cpp  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/\n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/\n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/.gitignore  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/Podfile.template  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/apple_package_test.xcodeproj/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/apple_package_test.xcodeproj/project.pbxproj  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/apple_package_test.xcodeproj/project.xcworkspace/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/apple_package_test.xcodeproj/project.xcworkspace/contents.xcworkspacedata  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/apple_package_test.xcodeproj/project.xcworkspace/xcshareddata/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/apple_package_test.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/apple_package_test.xcodeproj/project.xcworkspace/xcshareddata/WorkspaceSettings.xcsettings  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/AppDelegate.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/AppDelegate.m  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/Base.lproj/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/Base.lproj/LaunchScreen.storyboard  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/Base.lproj/Main.storyboard  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/Info.plist  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/ios_package_test.entitlements  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_test/main.m  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_testUITests/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/ios_package_testUITests/ios_package_uitest_cpp_api.mm  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_test/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_test/AppDelegate.h  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_test/AppDelegate.m  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_test/Base.lproj/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_test/Base.lproj/Main.storyboard  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_test/main.m  \n",
            "   creating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_testUITests/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/platform/apple/apple_package_test/macos_package_testUITests/macos_package_uitest_cpp_api.mm  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/_test_utils.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/conftest.py  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/cpu/\n",
            "   creating: onnxruntime-genai-0.11.2/test/python/cpu/ort/\n",
            " extracting: onnxruntime-genai-0.11.2/test/python/cpu/ort/requirements.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/cpu/torch/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/cpu/torch/requirements.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/cuda/\n",
            "   creating: onnxruntime-genai-0.11.2/test/python/cuda/ort/\n",
            " extracting: onnxruntime-genai-0.11.2/test/python/cuda/ort/requirements.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/cuda/torch/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/cuda/torch/requirements.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/directml/\n",
            "   creating: onnxruntime-genai-0.11.2/test/python/directml/ort/\n",
            " extracting: onnxruntime-genai-0.11.2/test/python/directml/ort/requirements.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/directml/torch/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/directml/torch/requirements.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/macos/\n",
            "   creating: onnxruntime-genai-0.11.2/test/python/macos/ort/\n",
            " extracting: onnxruntime-genai-0.11.2/test/python/macos/ort/requirements.txt  \n",
            "   creating: onnxruntime-genai-0.11.2/test/python/macos/torch/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/macos/torch/requirements.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/requirements.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/test_onnxruntime_genai.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/test_onnxruntime_genai_api.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/python/test_onnxruntime_genai_e2e.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/sampling_benchmark.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/sampling_tests.cpp  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/statistics_helper.h  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/\n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/added_tokens.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/audio_processor_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/dummy_decoder.onnx  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/dummy_encoder.onnx  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/genai_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/special_tokens_map.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/tokenizer.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/tokenizer_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audio-preprocessing/vocab.json  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/audios/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audios/1272-141231-0002.mp3  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/audios/jfk.flac  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/create_dummy_model.py  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/grammars/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/blog.sample.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/blog.schema.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/grammar_multiple_functions.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/grammar_multiple_inputs.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/grammar_read_files.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/weather_grammar.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/weather_population.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/grammars/weather_schema.json  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/\n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp16-cuda/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp16-cuda/genai_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp16-cuda/past.onnx  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32-cuda/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32-cuda/genai_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32-cuda/past.onnx  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32/genai_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32/past.onnx  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32/tokenizer.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32/tokenizer_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/hf-internal-testing/tiny-random-gpt2-fp32/vocab.json  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/images/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/images/10809054.jpg  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/images/australia.jpg  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/images/landscape.jpg  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/images/sheet.png  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/pipeline-model/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/pipeline-model/added_tokens.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/pipeline-model/genai_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/pipeline-model/special_tokens_map.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/pipeline-model/tokenizer.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/pipeline-model/tokenizer_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/pipeline-model/vocab.json  \n",
            "   creating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/\n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/dummy_embedding.onnx  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/dummy_text.onnx  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/dummy_vision.onnx  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/genai_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/processor_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/special_tokens_map.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/tokenizer.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/test_models/vision-preprocessing/tokenizer_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/tests_helper.cuh  \n",
            "  inflating: onnxruntime-genai-0.11.2/test/worker_thread_test.cpp  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/\n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/get_docker_image.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/\n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/android/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/android/build_aar_package.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/android/default_aar_build_settings.json  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/assemble_apple_packaging_artifacts.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/build_and_assemble_apple_pods.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/build_apple_framework.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/c/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/c/assemble_c_pod_package.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/c/c.podspec.template  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/c/onnxruntime-genai-c.config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/default_full_apple_framework_build_settings.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/default_full_ios_framework_build_settings.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/default_ios_simulator_apple_framework_build_settings.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/framework_info.json.template  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/get_simulator_device_info.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/objectivec/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/objectivec/assemble_objc_pod_package.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/objectivec/objc.podspec.template  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/objectivec/onnxruntime-genai-objc.config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/package_assembly_utils.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/apple/test_apple_packages.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/\n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/\n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/\n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/aarch64/\n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/aarch64/default/\n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/aarch64/default/cpu/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/aarch64/default/cpu/Dockerfile  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/aarch64/default/cpu/scripts/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/aarch64/default/cpu/scripts/install_centos.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/inference/aarch64/default/cpu/scripts/install_deps.sh  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/Dockerfile.manylinux2_28_cpu  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/Dockerfile.manylinux2_28_cuda_12.2  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/Dockerfile.manylinux2_28_rocm  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/manylinux.patch  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/scripts/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/scripts/install_centos.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/scripts/install_deps.sh  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/github/linux/docker/manylinux/scripts/requirements.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/ci_build/logger.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/nuget/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/nuget/generate_nuspec_for_custom_nuget.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/nuget/generate_nuspec_for_native_nuget.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/nuget/generate_nuspec_for_winml_nuget.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/python/\n",
            " extracting: onnxruntime-genai-0.11.2/tools/python/__init__.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/python/model_validation/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/model_validation/README.md  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/model_validation/perplexity_metrics.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/model_validation/requirements.txt  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/model_validation/validation_config.json  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/model_validation/validation_tool.py  \n",
            "   creating: onnxruntime-genai-0.11.2/tools/python/util/\n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/util/__init__.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/util/android.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/util/dependency_resolver.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/util/logger.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/util/platform_helpers.py  \n",
            "  inflating: onnxruntime-genai-0.11.2/tools/python/util/run.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YqVm0HEL3W10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwrRf5qA1sJR",
        "outputId": "cf807418-689e-47c6-dff2-8b74192b2141"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/onnxruntime-genai'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout rel-0.11.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iT-JEw01suh",
        "outputId": "2d7fed41-b06a-4812-89f9-b80fdd47a2a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNPbWL982OFt",
        "outputId": "2f19142a-3035-4411-cbda-78f39515e764"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-chat.py -m /content/1 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBu_YTQW3YRe",
        "outputId": "57bfe2e9-fafb-4a19-a31d-24c9a0a217a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Prompt (Use quit() to exit): hi\n",
            "\u001b[1;31m2025-11-18 21:28:59.422992972 [E:onnxruntime:onnxruntime-genai, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running Expand node. Name:'/model/attn_mask_reformat/input_ids_subgraph/Expand' Status Message: /model/attn_mask_reformat/input_ids_subgraph/Expand: left operand cannot broadcast on dim 3 LeftShape: {1,1,60,60}, RightShape: {1,1,60,72}\u001b[m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-chat.py\", line 235, in <module>\n",
            "    main(args)\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-chat.py\", line 178, in main\n",
            "    generator.append_tokens(input_tokens)\n",
            "RuntimeError: Non-zero status code returned while running Expand node. Name:'/model/attn_mask_reformat/input_ids_subgraph/Expand' Status Message: /model/attn_mask_reformat/input_ids_subgraph/Expand: left operand cannot broadcast on dim 3 LeftShape: {1,1,60,60}, RightShape: {1,1,60,72}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-generate.py -m /content/1 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6blQYw933mBA",
        "outputId": "b04f961b-59dc-4b87-9853-058c478f47b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Input: hi\n",
            "Prompt #0: hi\n",
            "\n",
            "hi\n",
            "\n",
            "\n",
            "Tokens: 1 Time: 0.15 Tokens per second: 6.62\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py"
      ],
      "metadata": {
        "id": "0PGQIj0y4Wkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-generate.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cMlLimZ4jCq",
        "outputId": "01ed99da-5dad-46aa-eb6a-e29b4e9f89f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "usage: model-generate.py [-h] -m MODEL_PATH\n",
            "                         [-e {cpu,cuda,dml,NvTensorRtRtx,follow_config}]\n",
            "                         [-pr [PROMPTS ...]] [-i MIN_LENGTH] [-l MAX_LENGTH]\n",
            "                         [-ds] [--top_p TOP_P] [-k TOP_K] [-t TEMPERATURE]\n",
            "                         [-r REPETITION_PENALTY] [-v]\n",
            "                         [-b BATCH_SIZE_FOR_CUDA_GRAPH] [-c CHAT_TEMPLATE]\n",
            "                         [--chunk_size CHUNK_SIZE] [-n NUM_BEAMS]\n",
            "                         [--non-interactive | --no-non-interactive]\n",
            "\n",
            "End-to-end token generation loop example for gen-ai\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -m MODEL_PATH, --model_path MODEL_PATH\n",
            "                        Onnx model folder path (must contain genai_config.json\n",
            "                        and model.onnx)\n",
            "  -e {cpu,cuda,dml,NvTensorRtRtx,follow_config}, --execution_provider {cpu,cuda,dml,NvTensorRtRtx,follow_config}\n",
            "                        Execution provider to run the ONNX Runtime session\n",
            "                        with. Defaults to follow_config that uses the\n",
            "                        execution provider listed in the genai_config.json\n",
            "                        instead.\n",
            "  -pr [PROMPTS ...], --prompts [PROMPTS ...]\n",
            "                        Input prompts to generate tokens from. Provide this\n",
            "                        parameter multiple times to batch multiple prompts\n",
            "  -i MIN_LENGTH, --min_length MIN_LENGTH\n",
            "                        Min number of tokens to generate including the prompt\n",
            "  -l MAX_LENGTH, --max_length MAX_LENGTH\n",
            "                        Max number of tokens to generate including the prompt\n",
            "  -ds, --do_sample      Do random sampling. When false, greedy or beam search\n",
            "                        are used to generate the output. Defaults to false\n",
            "  --top_p TOP_P         Top p probability to sample with\n",
            "  -k TOP_K, --top_k TOP_K\n",
            "                        Top k tokens to sample from\n",
            "  -t TEMPERATURE, --temperature TEMPERATURE\n",
            "                        Temperature to sample with\n",
            "  -r REPETITION_PENALTY, --repetition_penalty REPETITION_PENALTY\n",
            "                        Repetition penalty to sample with\n",
            "  -v, --verbose         Print verbose output and timing information. Defaults\n",
            "                        to false\n",
            "  -b BATCH_SIZE_FOR_CUDA_GRAPH, --batch_size_for_cuda_graph BATCH_SIZE_FOR_CUDA_GRAPH\n",
            "                        Max batch size for CUDA graph\n",
            "  -c CHAT_TEMPLATE, --chat_template CHAT_TEMPLATE\n",
            "                        Chat template to use for the prompt. User input will\n",
            "                        be injected into {input}. If not set, the prompt is\n",
            "                        used as is.\n",
            "  --chunk_size CHUNK_SIZE\n",
            "                        Chunk size for prefill chunking during context\n",
            "                        processing (default: 0 = disabled, >0 = enabled)\n",
            "  -n NUM_BEAMS, --num_beams NUM_BEAMS\n",
            "                        Number of beams for beam search (default: 3)\n",
            "  --non-interactive, --no-non-interactive\n",
            "                        Non-interactive mode, mainly for CI usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-generate.py -m /content/1 -e cuda -pr hi -l 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY-kdBUl4k0n",
        "outputId": "afd73a57-6ad0-4026-ed02-d1c7125385e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Prompt #0: hi\n",
            "\n",
            "hi\n",
            "\n",
            "\n",
            "Tokens: 1 Time: 0.09 Tokens per second: 11.23\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-generate.py -m /content/1 -e cuda -pr \"what is ai?\" -l 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_KF9CWM5Bz1",
        "outputId": "5ddf8f19-62e4-4300-b037-bd91f58519bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Prompt #0: what is ai?\n",
            "\n",
            "what is ai?\n",
            "\n",
            "\n",
            "Tokens: 4 Time: 0.11 Tokens per second: 35.64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-generate.py -m /content/1 -e cuda -c '<|user|>\\n{input} <|end|>\\n<|assistant|>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8Mwu3ES5If9",
        "outputId": "ffc0bf2f-f416-4ae4-925d-de7697efb348"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: what is python?\n",
            "Error, chat template must have exactly one pair of curly braces, e.g. '<|user|>\n",
            "{input} <|end|>\n",
            "<|assistant|>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-generate.py -m /content/1 -e cuda -c '<|user|>{input}<|end|><|assistant|>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTLAbCY65eqO",
        "outputId": "a6a889fb-bfb4-48bc-a8cf-b62727602f44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: what is python?\n",
            "Error, chat template must have exactly one pair of curly braces, e.g. '<|user|>\n",
            "{input} <|end|>\n",
            "<|assistant|>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime_genai as og\n",
        "import sys\n",
        "\n",
        "# **1. تحديد المسار ومزود التنفيذ (CUDA)**\n",
        "model_path = '/content/1'  # يجب أن يتطابق مع المسار /content/1 الذي استخدمته\n",
        "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'\n",
        "\n",
        "config = og.Config(model_path)\n",
        "config.set_execution_provider(\"cuda\") # الحل لمشكلة 'Unknown provider name'\n",
        "\n",
        "try:\n",
        "    model = og.Model(config)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Failed to load model: {e}\")\n",
        "    sys.exit()\n",
        "\n",
        "tokenizer = og.Tokenizer(model)\n",
        "tokenizer_stream = tokenizer.create_stream()\n",
        "\n",
        "# 2. إعدادات Generator\n",
        "search_options = {}\n",
        "search_options['max_length'] = 2048\n",
        "search_options['batch_size'] = 1\n",
        "\n",
        "# 3. حلقة الدردشة\n",
        "while True:\n",
        "    text = input(\"Input: \")\n",
        "    if text.lower() in ['quit', 'exit', 'quit()', 'exit()']:\n",
        "        break\n",
        "    if not text:\n",
        "        print(\"Error, input cannot be empty\")\n",
        "        continue\n",
        "\n",
        "    # تطبيق قالب الدردشة\n",
        "    prompt = f'{chat_template.format(input=text)}'\n",
        "\n",
        "    input_tokens = tokenizer.encode(prompt)\n",
        "\n",
        "    params = og.GeneratorParams(model)\n",
        "    params.set_search_options(**search_options)\n",
        "\n",
        "    # لضمان عدم إعادة استخدام Generator في حلقة، قم بإنشائه داخل الحلقة\n",
        "    generator = og.Generator(model, params)\n",
        "\n",
        "    print(\"Output: \", end='', flush=True)\n",
        "\n",
        "    try:\n",
        "        generator.append_tokens(input_tokens)\n",
        "        while True:\n",
        "            generator.generate_next_token()\n",
        "            if generator.is_done():\n",
        "                break\n",
        "            new_token = generator.get_next_tokens()[0]\n",
        "            print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"  --control+c pressed, aborting generation--\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nGeneration Error: {e}\")\n",
        "\n",
        "    print()\n",
        "    del generator # تحرير ذاكرة Generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "J3tEClwD52oC",
        "outputId": "a4bfdac9-bd79-43df-c650-f5686b470523"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'onnxruntime_genai.onnxruntime_genai.Config' object has no attribute 'set_execution_provider'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3483381688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_execution_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# الحل لمشكلة 'Unknown provider name'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'onnxruntime_genai.onnxruntime_genai.Config' object has no attribute 'set_execution_provider'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-chat.py -m /content/1 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpuodFW96CM1",
        "outputId": "7f504f1a-2b9a-4e2e-f13a-faab1c82de1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt (Use quit() to exit): hi\n",
            "\u001b[1;31m2025-11-18 21:40:19.741637829 [E:onnxruntime:onnxruntime-genai, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running Expand node. Name:'/model/attn_mask_reformat/input_ids_subgraph/Expand' Status Message: /model/attn_mask_reformat/input_ids_subgraph/Expand: left operand cannot broadcast on dim 3 LeftShape: {1,1,60,60}, RightShape: {1,1,60,72}\u001b[m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-chat.py\", line 235, in <module>\n",
            "    main(args)\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-chat.py\", line 178, in main\n",
            "    generator.append_tokens(input_tokens)\n",
            "RuntimeError: Non-zero status code returned while running Expand node. Name:'/model/attn_mask_reformat/input_ids_subgraph/Expand' Status Message: /model/attn_mask_reformat/input_ids_subgraph/Expand: left operand cannot broadcast on dim 3 LeftShape: {1,1,60,60}, RightShape: {1,1,60,72}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/onnxruntime-genai-0.11.2/examples/python/model-qa.py"
      ],
      "metadata": {
        "id": "MtQSNZuL6MA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqLEKyPp6kHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m /content/1 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0WD86rj6Y4D",
        "outputId": "19420b0a-7d99-4572-c1b1-58d4aa6938e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt (Use quit() to exit): hi\n",
            "\n",
            "Output: \n",
            "  --control+c pressed, aborting generation--\n",
            "\n",
            "\n",
            "Prompt (Use quit() to exit): Error, input cannot be empty\n",
            "Prompt (Use quit() to exit): what are you?\n",
            "\n",
            "Output: \n",
            "\n",
            "\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/onnxruntime-genai-0.11.2/examples/python/guidance-example.py"
      ],
      "metadata": {
        "id": "OO4czPt66a2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -m /content/1 -e cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQVZixkd7jH9",
        "outputId": "1dfbc3cb-dd9f-43f1-8cdc-6b5c799697a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Prompt (Use quit() to exit): how are you?\n",
            "\n",
            "Output:   --control+c pressed, aborting generation--\n",
            "\n",
            "\n",
            "Prompt (Use quit() to exit): Traceback (most recent call last):\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9h109CDuDBYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eqFYZsRqDHdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/2\n",
        "!huggingface-cli download Prince-1/TinyLlama-1.1B-Chat-v1.0-Onnx --local-dir /content/2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fLjGcGC_Mk8",
        "outputId": "d196c95d-c8f4-4030-d68e-fec46db6f601"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/2\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Fetching 8 files:   0% 0/8 [00:00<?, ?it/s]Downloading 'tokenizer.model' to '/content/2/.cache/huggingface/download/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347.incomplete'\n",
            "Downloading 'model.onnx' to '/content/2/.cache/huggingface/download/ihhw_uFzBe-Y54_HOJQmXx4GS8A=.9a5fba2809456005858e3c8845d75fb4b18b2bd51ebb8174baf61fc870389689.incomplete'\n",
            "Downloading 'model.onnx.data' to '/content/2/.cache/huggingface/download/Lzxw0O44RMmhTgKWeQQ-MMzHFnE=.5be7b303218b6358f4fb80e92c6b2f4f847ac20309e9f3948896b9397101be4e.incomplete'\n",
            "Downloading 'tokenizer.json' to '/content/2/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.6b684f885f88f239c3ce65d728241672132152b6.incomplete'\n",
            "Downloading '.gitattributes' to '/content/2/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.705765cea3520406fae64d16b6da31b72948a9a0.incomplete'\n",
            "Downloading 'special_tokens_map.json' to '/content/2/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.492d4b2966a1763442d426d880dbc29f94906e4c.incomplete'\n",
            "Downloading 'genai_config.json' to '/content/2/.cache/huggingface/download/yDyKFBRCOr3XTWAm4EHNQUX0s3Y=.1cd8af4bc99f20959e3c9f123bcdefdf22457f21.incomplete'\n",
            "\n",
            "model.onnx.data:   0% 0.00/2.20G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model.onnx:   0% 0.00/518k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.model:   0% 0.00/500k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'tokenizer_config.json' to '/content/2/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.3113ed77ecf056c0966e6d8ff18b2dfe1ff1a27c.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 1.57kB [00:00, 8.54MB/s]\n",
            "Download complete. Moving file to /content/2/.gitattributes\n",
            "Fetching 8 files:  12% 1/8 [00:00<00:02,  2.54it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "special_tokens_map.json: 100% 551/551 [00:00<00:00, 5.53MB/s]\n",
            "Download complete. Moving file to /content/2/special_tokens_map.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "genai_config.json: 1.50kB [00:00, 7.34MB/s]\n",
            "Download complete. Moving file to /content/2/genai_config.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 3.62MB [00:00, 31.7MB/s]\n",
            "Download complete. Moving file to /content/2/tokenizer.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 1.40kB [00:00, 6.21MB/s]\n",
            "Download complete. Moving file to /content/2/tokenizer_config.json\n",
            "\n",
            "\n",
            "model.onnx: 100% 518k/518k [00:01<00:00, 451kB/s]\n",
            "Download complete. Moving file to /content/2/model.onnx\n",
            "Fetching 8 files:  38% 3/8 [00:01<00:02,  2.01it/s]\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 426kB/s]\n",
            "Download complete. Moving file to /content/2/tokenizer.model\n",
            "\n",
            "model.onnx.data:   0% 33.4k/2.20G [00:04<89:55:15, 6.80kB/s]\u001b[A\n",
            "model.onnx.data:   0% 73.4k/2.20G [00:05<34:53:27, 17.5kB/s]\u001b[A\n",
            "model.onnx.data:   0% 3.17M/2.20G [00:05<30:53, 1.19MB/s]   \u001b[A\n",
            "model.onnx.data:   1% 12.3M/2.20G [00:05<06:18, 5.78MB/s]\u001b[A\n",
            "model.onnx.data:   1% 24.7M/2.20G [00:05<02:39, 13.7MB/s]\u001b[A\n",
            "model.onnx.data:   1% 32.0M/2.20G [00:05<02:30, 14.4MB/s]\u001b[A\n",
            "model.onnx.data:   2% 40.4M/2.20G [00:05<01:45, 20.5MB/s]\u001b[A\n",
            "model.onnx.data:   2% 46.4M/2.20G [00:06<01:28, 24.2MB/s]\u001b[A\n",
            "model.onnx.data:   2% 52.3M/2.20G [00:06<01:29, 23.9MB/s]\u001b[A\n",
            "model.onnx.data:   3% 58.7M/2.20G [00:06<01:22, 25.9MB/s]\u001b[A\n",
            "model.onnx.data:   3% 63.3M/2.20G [00:06<01:16, 27.8MB/s]\u001b[A\n",
            "model.onnx.data:   6% 140M/2.20G [00:06<00:14, 139MB/s]  \u001b[A\n",
            "model.onnx.data:   7% 158M/2.20G [00:07<00:24, 84.6MB/s]\u001b[A\n",
            "model.onnx.data:   8% 173M/2.20G [00:07<00:24, 81.3MB/s]\u001b[A\n",
            "model.onnx.data:   8% 186M/2.20G [00:07<00:27, 73.4MB/s]\u001b[A\n",
            "model.onnx.data:   9% 196M/2.20G [00:07<00:27, 73.1MB/s]\u001b[A\n",
            "model.onnx.data:   9% 206M/2.20G [00:08<00:29, 68.5MB/s]\u001b[A\n",
            "model.onnx.data:  10% 214M/2.20G [00:08<00:30, 64.7MB/s]\u001b[A\n",
            "model.onnx.data:  10% 221M/2.20G [00:08<00:33, 58.3MB/s]\u001b[A\n",
            "model.onnx.data:  10% 229M/2.20G [00:08<00:32, 60.7MB/s]\u001b[A\n",
            "model.onnx.data:  11% 244M/2.20G [00:08<00:24, 79.0MB/s]\u001b[A\n",
            "model.onnx.data:  12% 253M/2.20G [00:08<00:24, 79.5MB/s]\u001b[A\n",
            "model.onnx.data:  12% 264M/2.20G [00:08<00:22, 86.5MB/s]\u001b[A\n",
            "model.onnx.data:  12% 274M/2.20G [00:08<00:22, 85.9MB/s]\u001b[A\n",
            "model.onnx.data:  13% 284M/2.20G [00:09<00:25, 76.5MB/s]\u001b[A\n",
            "model.onnx.data:  13% 293M/2.20G [00:09<00:24, 77.1MB/s]\u001b[A\n",
            "model.onnx.data:  14% 304M/2.20G [00:09<00:22, 85.6MB/s]\u001b[A\n",
            "model.onnx.data:  14% 314M/2.20G [00:09<00:21, 88.3MB/s]\u001b[A\n",
            "model.onnx.data:  15% 324M/2.20G [00:09<00:20, 90.9MB/s]\u001b[A\n",
            "model.onnx.data:  15% 335M/2.20G [00:09<00:19, 96.9MB/s]\u001b[A\n",
            "model.onnx.data:  16% 346M/2.20G [00:09<00:24, 76.8MB/s]\u001b[A\n",
            "model.onnx.data:  16% 356M/2.20G [00:09<00:22, 81.2MB/s]\u001b[A\n",
            "model.onnx.data:  17% 368M/2.20G [00:10<00:20, 90.4MB/s]\u001b[A\n",
            "model.onnx.data:  18% 385M/2.20G [00:10<00:16, 111MB/s] \u001b[A\n",
            "model.onnx.data:  18% 397M/2.20G [00:10<00:18, 98.7MB/s]\u001b[A\n",
            "model.onnx.data:  19% 409M/2.20G [00:10<00:19, 91.1MB/s]\u001b[A\n",
            "model.onnx.data:  19% 425M/2.20G [00:10<00:18, 93.8MB/s]\u001b[A\n",
            "model.onnx.data:  23% 505M/2.20G [00:10<00:06, 252MB/s] \u001b[A\n",
            "model.onnx.data:  27% 587M/2.20G [00:10<00:04, 340MB/s]\u001b[A\n",
            "model.onnx.data:  28% 623M/2.20G [00:11<00:05, 269MB/s]\u001b[A\n",
            "model.onnx.data:  30% 654M/2.20G [00:11<00:05, 261MB/s]\u001b[A\n",
            "model.onnx.data:  31% 682M/2.20G [00:11<00:08, 189MB/s]\u001b[A\n",
            "model.onnx.data:  32% 705M/2.20G [00:11<00:09, 165MB/s]\u001b[A\n",
            "model.onnx.data:  33% 730M/2.20G [00:11<00:08, 179MB/s]\u001b[A\n",
            "model.onnx.data:  34% 751M/2.20G [00:11<00:07, 182MB/s]\u001b[A\n",
            "model.onnx.data:  35% 778M/2.20G [00:12<00:07, 200MB/s]\u001b[A\n",
            "model.onnx.data:  36% 802M/2.20G [00:12<00:06, 209MB/s]\u001b[A\n",
            "model.onnx.data:  38% 825M/2.20G [00:12<00:06, 205MB/s]\u001b[A\n",
            "model.onnx.data:  38% 847M/2.20G [00:12<00:07, 185MB/s]\u001b[A\n",
            "model.onnx.data:  39% 867M/2.20G [00:16<01:17, 17.3MB/s]\u001b[A\n",
            "model.onnx.data:  49% 1.07G/2.20G [00:17<00:15, 70.7MB/s]\u001b[A\n",
            "model.onnx.data:  50% 1.09G/2.20G [00:17<00:15, 70.6MB/s]\u001b[A\n",
            "model.onnx.data:  51% 1.11G/2.20G [00:18<00:17, 61.9MB/s]\u001b[A\n",
            "model.onnx.data:  51% 1.13G/2.20G [00:18<00:19, 55.0MB/s]\u001b[A\n",
            "model.onnx.data:  53% 1.16G/2.20G [00:18<00:13, 74.1MB/s]\u001b[A\n",
            "model.onnx.data:  54% 1.18G/2.20G [00:19<00:16, 60.0MB/s]\u001b[A\n",
            "model.onnx.data:  54% 1.19G/2.20G [00:19<00:17, 59.0MB/s]\u001b[A\n",
            "model.onnx.data:  55% 1.21G/2.20G [00:19<00:17, 56.3MB/s]\u001b[A\n",
            "model.onnx.data:  55% 1.21G/2.20G [00:19<00:18, 53.7MB/s]\u001b[A\n",
            "model.onnx.data:  56% 1.22G/2.20G [00:20<00:18, 52.3MB/s]\u001b[A\n",
            "model.onnx.data:  56% 1.23G/2.20G [00:20<00:18, 52.0MB/s]\u001b[A\n",
            "model.onnx.data:  56% 1.23G/2.20G [00:20<00:18, 52.8MB/s]\u001b[A\n",
            "model.onnx.data:  56% 1.24G/2.20G [00:20<00:20, 46.1MB/s]\u001b[A\n",
            "model.onnx.data:  60% 1.32G/2.20G [00:20<00:05, 173MB/s] \u001b[A\n",
            "model.onnx.data:  61% 1.35G/2.20G [00:21<00:07, 116MB/s]\u001b[A\n",
            "model.onnx.data:  62% 1.37G/2.20G [00:21<00:07, 112MB/s]\u001b[A\n",
            "model.onnx.data:  63% 1.39G/2.20G [00:21<00:08, 90.5MB/s]\u001b[A\n",
            "model.onnx.data:  64% 1.40G/2.20G [00:21<00:09, 82.8MB/s]\u001b[A\n",
            "model.onnx.data:  64% 1.41G/2.20G [00:21<00:09, 86.7MB/s]\u001b[A\n",
            "model.onnx.data:  65% 1.42G/2.20G [00:22<00:08, 90.5MB/s]\u001b[A\n",
            "model.onnx.data:  65% 1.43G/2.20G [00:22<00:08, 90.2MB/s]\u001b[A\n",
            "model.onnx.data:  66% 1.45G/2.20G [00:22<00:09, 78.1MB/s]\u001b[A\n",
            "model.onnx.data:  66% 1.46G/2.20G [00:22<00:09, 78.9MB/s]\u001b[A\n",
            "model.onnx.data:  67% 1.46G/2.20G [00:22<00:08, 82.5MB/s]\u001b[A\n",
            "model.onnx.data:  67% 1.48G/2.20G [00:22<00:07, 92.7MB/s]\u001b[A\n",
            "model.onnx.data:  68% 1.49G/2.20G [00:22<00:06, 102MB/s] \u001b[A\n",
            "model.onnx.data:  68% 1.50G/2.20G [00:22<00:08, 82.3MB/s]\u001b[A\n",
            "model.onnx.data:  69% 1.52G/2.20G [00:23<00:07, 96.5MB/s]\u001b[A\n",
            "model.onnx.data:  70% 1.54G/2.20G [00:23<00:05, 126MB/s] \u001b[A\n",
            "model.onnx.data:  71% 1.55G/2.20G [00:23<00:05, 128MB/s]\u001b[A\n",
            "model.onnx.data:  71% 1.57G/2.20G [00:23<00:09, 70.0MB/s]\u001b[A\n",
            "model.onnx.data:  72% 1.59G/2.20G [00:23<00:06, 94.9MB/s]\u001b[A\n",
            "model.onnx.data:  73% 1.61G/2.20G [00:23<00:05, 112MB/s] \u001b[A\n",
            "model.onnx.data:  74% 1.63G/2.20G [00:24<00:04, 132MB/s]\u001b[A\n",
            "model.onnx.data:  75% 1.65G/2.20G [00:24<00:05, 110MB/s]\u001b[A\n",
            "model.onnx.data:  76% 1.66G/2.20G [00:24<00:04, 112MB/s]\u001b[A\n",
            "model.onnx.data:  77% 1.68G/2.20G [00:24<00:03, 135MB/s]\u001b[A\n",
            "model.onnx.data:  78% 1.71G/2.20G [00:24<00:02, 164MB/s]\u001b[A\n",
            "model.onnx.data:  79% 1.73G/2.20G [00:24<00:02, 174MB/s]\u001b[A\n",
            "model.onnx.data:  83% 1.82G/2.20G [00:24<00:01, 293MB/s]\u001b[A\n",
            "model.onnx.data:  84% 1.84G/2.20G [00:25<00:01, 262MB/s]\u001b[A\n",
            "model.onnx.data:  85% 1.87G/2.20G [00:25<00:01, 263MB/s]\u001b[A\n",
            "model.onnx.data:  86% 1.90G/2.20G [00:25<00:01, 263MB/s]\u001b[A\n",
            "model.onnx.data:  88% 1.93G/2.20G [00:25<00:01, 270MB/s]\u001b[A\n",
            "model.onnx.data:  89% 1.96G/2.20G [00:25<00:00, 259MB/s]\u001b[A\n",
            "model.onnx.data:  90% 1.98G/2.20G [00:25<00:01, 187MB/s]\u001b[A\n",
            "model.onnx.data:  91% 2.00G/2.20G [00:25<00:01, 161MB/s]\u001b[A\n",
            "model.onnx.data:  95% 2.09G/2.20G [00:25<00:00, 309MB/s]\u001b[A\n",
            "model.onnx.data:  97% 2.13G/2.20G [00:26<00:00, 310MB/s]\u001b[A\n",
            "model.onnx.data: 100% 2.20G/2.20G [00:26<00:00, 81.9MB/s]\n",
            "Download complete. Moving file to /content/2/model.onnx.data\n",
            "Fetching 8 files: 100% 8/8 [00:27<00:00,  3.40s/it]\n",
            "/content/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -m /content/2 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D09dSHmx_enN",
        "outputId": "dee96261-211b-4f18-be24-a88f0bceb0aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Prompt (Use quit() to exit): hi\n",
            "\n",
            "Output: I'm sorry to hear that you're having trouble with the AI assistant. Please provide me with more details about the issue you're experiencing. What specific actions or commands are you trying to execute, and what is the output you're seeing? Also, can you provide me with any error messages or logs that may help me diagnose the issue?\n",
            "\n",
            "If possible, please provide me with the steps you've taken to troubleshoot the issue. This will help me understand the context of the problem and identify any potential causes.\n",
            "\n",
            "Thank you for your cooperation. I'll do my best to help you resolve the issue.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "[Your Company]\n",
            "\n",
            "Prompt (Use quit() to exit): what is python?\n",
            "\n",
            "Output: Python is a high-level, interpreted, interactive, and object-oriented programming language. It is widely used for web development, data science, machine learning, artificial intelligence, and many other fields. Python is designed to be easy to learn and use, with a simple syntax and a focus on readability and clarity.\n",
            "\n",
            "Python is a popular choice for web development because it is easy to use and has a large community of developers who provide support and resources. Python is also widely used in data science, machine learning, and artificial intelligence, where it is known for its flexibility, speed, and ability to handle large datasets.\n",
            "\n",
            "Python is a popular choice for web development because it is easy to use and has a large community of developers who provide support and resources.\n",
            "\n",
            "In summary, Python is a popular programming language used for web development, data science, machine learning, artificial intelligence, and many other fields.\n",
            "\n",
            "Prompt (Use quit() to exit): quit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -m /content/1 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIgraCF1_r_e",
        "outputId": "59d7d2f0-88ba-4fce-855b-60df5848918e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Prompt (Use quit() to exit): hi\n",
            "\n",
            "Output: ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/3\n",
        "!huggingface-cli download Prince-1/Medgemma-4b-pt-Onnx --local-dir /content/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKgdUYj0AdVl",
        "outputId": "053d13c0-c53b-4222-8004-d4967748eefd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/3\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Fetching 10 files:   0% 0/10 [00:00<?, ?it/s]Downloading 'tokenizer.json' to '/content/3/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.4667f2089529e8e7657cfb6d1c19910ae71ff5f28aa7ab2ff2763330affad795.incomplete'\n",
            "Downloading 'model.onnx.data' to '/content/3/.cache/huggingface/download/Lzxw0O44RMmhTgKWeQQ-MMzHFnE=.cfef8b96435cd84367c2ab8ab8d442d0acb2aad7ff25487490744ea8512abf34.incomplete'\n",
            "Downloading 'model.onnx' to '/content/3/.cache/huggingface/download/ihhw_uFzBe-Y54_HOJQmXx4GS8A=.9cad30c1880b227f881111730256e62c197c3131bd88ef08950217f895f55e65.incomplete'\n",
            "\n",
            "tokenizer.json:   0% 0.00/33.4M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model.onnx.data:   0% 0.00/9.24G [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'README.md' to '/content/3/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.ee86486575c4e36fb7e2c581af40c887ac61b72d.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "model.onnx:   0% 0.00/1.01M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'added_tokens.json' to '/content/3/.cache/huggingface/download/SeqzFlf9ZNZ3or_wZAOIdsM3Yxw=.e17bde03d42feda32d1abfca6d3b598b9a020df7.incomplete'\n",
            "Downloading 'special_tokens_map.json' to '/content/3/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.1a6193244714d3d78be48666cb02cdbfac62ad86.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "README.md: 26.9kB [00:00, 49.5MB/s]\n",
            "Download complete. Moving file to /content/3/README.md\n",
            "Downloading 'genai_config.json' to '/content/3/.cache/huggingface/download/yDyKFBRCOr3XTWAm4EHNQUX0s3Y=.6b6cb4261e05ac321c15672dcad9b10eb65dcfff.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "added_tokens.json: 100% 35.0/35.0 [00:00<00:00, 386kB/s]\n",
            "Download complete. Moving file to /content/3/added_tokens.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "special_tokens_map.json: 100% 662/662 [00:00<00:00, 7.27MB/s]\n",
            "Download complete. Moving file to /content/3/special_tokens_map.json\n",
            "Downloading '.gitattributes' to '/content/3/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.ce10c6f94e389e3a6bb0a424df4522771164485a.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "genai_config.json: 1.51kB [00:00, 6.76MB/s]\n",
            "Download complete. Moving file to /content/3/genai_config.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 1.62kB [00:00, 11.1MB/s]\n",
            "Download complete. Moving file to /content/3/.gitattributes\n",
            "Fetching 10 files:  10% 1/10 [00:00<00:04,  1.96it/s]Downloading 'tokenizer_config.json' to '/content/3/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.45c3df5b040e69e2a793ef94f4494e645247b87e.incomplete'\n",
            "Downloading 'tokenizer.model' to '/content/3/.cache/huggingface/download/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.1299c11d7cf632ef3b4e11937501358ada021bbdf7c47638d13c0ee982f2e79c.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model:   0% 0.00/4.69M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 1.16MB [00:00, 57.0MB/s]\n",
            "Download complete. Moving file to /content/3/tokenizer_config.json\n",
            "\n",
            "tokenizer.json: 100% 33.4M/33.4M [00:00<00:00, 41.4MB/s]\n",
            "Download complete. Moving file to /content/3/tokenizer.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 4.69M/4.69M [00:00<00:00, 11.2MB/s]\n",
            "Download complete. Moving file to /content/3/tokenizer.model\n",
            "\n",
            "\n",
            "\n",
            "model.onnx: 100% 1.01M/1.01M [00:01<00:00, 849kB/s]\n",
            "Download complete. Moving file to /content/3/model.onnx\n",
            "Fetching 10 files:  50% 5/10 [00:01<00:01,  3.68it/s]\n",
            "\n",
            "model.onnx.data:   0% 566k/9.24G [00:01<7:45:51, 330kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   0% 18.6M/9.24G [00:01<10:59, 14.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   0% 38.7M/9.24G [00:02<07:34, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   2% 168M/9.24G [00:03<01:50, 81.9MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   3% 242M/9.24G [00:05<02:43, 55.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   3% 304M/9.24G [00:05<01:54, 77.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   4% 371M/9.24G [00:05<01:28, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   5% 438M/9.24G [00:06<01:17, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   5% 505M/9.24G [00:06<01:13, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   6% 572M/9.24G [00:07<01:10, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   7% 639M/9.24G [00:08<01:48, 79.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   7% 687M/9.24G [00:08<01:35, 89.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   8% 720M/9.24G [00:09<01:41, 84.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   8% 781M/9.24G [00:09<01:14, 113MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:   9% 848M/9.24G [00:10<01:09, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  10% 915M/9.24G [00:10<00:57, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  11% 980M/9.24G [00:11<01:37, 85.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  14% 1.28G/9.24G [00:14<01:28, 90.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  15% 1.41G/9.24G [00:15<01:04, 121MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  16% 1.48G/9.24G [00:15<01:00, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  17% 1.55G/9.24G [00:21<02:55, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  18% 1.68G/9.24G [00:21<01:54, 65.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  19% 1.75G/9.24G [00:21<01:36, 78.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  20% 1.81G/9.24G [00:22<01:32, 80.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  20% 1.81G/9.24G [00:39<01:32, 80.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  20% 1.88G/9.24G [01:10<22:46, 5.39MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  21% 1.95G/9.24G [01:10<16:45, 7.25MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  22% 2.01G/9.24G [01:10<12:12, 9.86MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model.onnx.data:  23% 2.08G/9.24G [01:13<10:17, 11.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  23% 2.15G/9.24G [01:16<08:36, 13.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  24% 2.22G/9.24G [01:16<06:08, 19.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  25% 2.28G/9.24G [01:16<04:25, 26.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  25% 2.35G/9.24G [01:17<03:13, 35.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  26% 2.42G/9.24G [01:17<02:23, 47.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  27% 2.48G/9.24G [01:17<01:46, 63.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  28% 2.55G/9.24G [01:17<01:23, 80.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  28% 2.62G/9.24G [01:18<01:04, 103MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  29% 2.69G/9.24G [01:18<00:52, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  30% 2.75G/9.24G [01:18<00:44, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  31% 2.82G/9.24G [01:19<00:37, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  31% 2.89G/9.24G [01:19<00:34, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  32% 2.95G/9.24G [01:19<00:30, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  33% 3.02G/9.24G [01:19<00:28, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  33% 3.09G/9.24G [01:20<00:26, 232MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  34% 3.15G/9.24G [01:20<00:27, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  35% 3.22G/9.24G [01:20<00:22, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  36% 3.29G/9.24G [01:20<00:26, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  36% 3.36G/9.24G [01:21<00:21, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  37% 3.42G/9.24G [01:21<00:33, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  38% 3.49G/9.24G [01:22<00:35, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  39% 3.56G/9.24G [01:22<00:38, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  39% 3.62G/9.24G [01:23<00:35, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  40% 3.69G/9.24G [01:23<00:37, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  41% 3.76G/9.24G [01:24<00:44, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  41% 3.83G/9.24G [01:26<01:13, 73.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  42% 3.89G/9.24G [01:26<00:56, 94.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  43% 3.96G/9.24G [01:26<00:45, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  44% 4.03G/9.24G [01:27<00:48, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  44% 4.09G/9.24G [01:27<00:41, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  45% 4.16G/9.24G [01:28<00:35, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  46% 4.23G/9.24G [01:28<00:27, 181MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  46% 4.29G/9.24G [01:28<00:21, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  47% 4.36G/9.24G [01:28<00:17, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  48% 4.43G/9.24G [01:28<00:16, 287MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  49% 4.50G/9.24G [01:28<00:17, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  49% 4.56G/9.24G [01:29<00:17, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  50% 4.63G/9.24G [01:29<00:17, 266MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  51% 4.70G/9.24G [01:29<00:17, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  52% 4.76G/9.24G [01:30<00:17, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  52% 4.83G/9.24G [01:30<00:17, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  53% 4.90G/9.24G [01:30<00:17, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  54% 4.97G/9.24G [01:30<00:16, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  54% 5.03G/9.24G [01:31<00:25, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  55% 5.10G/9.24G [01:31<00:20, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  56% 5.17G/9.24G [01:32<00:25, 158MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  57% 5.23G/9.24G [01:32<00:23, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  57% 5.30G/9.24G [01:32<00:20, 192MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  58% 5.37G/9.24G [01:33<00:17, 218MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  59% 5.43G/9.24G [01:33<00:17, 223MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  60% 5.57G/9.24G [01:33<00:14, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  61% 5.64G/9.24G [01:34<00:12, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  62% 5.70G/9.24G [01:34<00:15, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  62% 5.77G/9.24G [01:34<00:13, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  63% 5.84G/9.24G [01:36<00:30, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  64% 5.90G/9.24G [01:36<00:25, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  65% 5.97G/9.24G [01:36<00:22, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  65% 6.04G/9.24G [01:36<00:17, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  66% 6.11G/9.24G [01:37<00:18, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  68% 6.24G/9.24G [01:37<00:13, 214MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  68% 6.31G/9.24G [01:37<00:12, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  69% 6.37G/9.24G [01:38<00:11, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  70% 6.44G/9.24G [01:38<00:11, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  70% 6.51G/9.24G [01:38<00:11, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  71% 6.57G/9.24G [01:38<00:10, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  72% 6.64G/9.24G [01:41<00:31, 82.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  73% 6.71G/9.24G [01:41<00:22, 110MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  73% 6.78G/9.24G [01:41<00:18, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  74% 6.84G/9.24G [01:41<00:14, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  75% 6.91G/9.24G [01:41<00:11, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  76% 6.98G/9.24G [01:42<00:10, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  76% 7.04G/9.24G [01:42<00:09, 228MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  77% 7.11G/9.24G [01:42<00:09, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  78% 7.18G/9.24G [01:42<00:08, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  78% 7.25G/9.24G [01:43<00:08, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  79% 7.31G/9.24G [01:43<00:09, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  80% 7.38G/9.24G [01:43<00:08, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  81% 7.45G/9.24G [01:44<00:07, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  81% 7.51G/9.24G [01:44<00:07, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  82% 7.58G/9.24G [01:44<00:07, 233MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  83% 7.65G/9.24G [01:45<00:07, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  84% 7.72G/9.24G [01:45<00:06, 228MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  84% 7.78G/9.24G [01:45<00:05, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  85% 7.85G/9.24G [01:45<00:05, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  86% 7.91G/9.24G [01:46<00:06, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  86% 7.98G/9.24G [01:46<00:06, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  87% 8.05G/9.24G [01:46<00:05, 205MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  88% 8.12G/9.24G [01:47<00:04, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  88% 8.16G/9.24G [01:47<00:04, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  89% 8.23G/9.24G [01:47<00:03, 306MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  90% 8.30G/9.24G [01:47<00:03, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  91% 8.37G/9.24G [01:51<00:17, 51.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  91% 8.43G/9.24G [01:51<00:12, 63.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  93% 8.57G/9.24G [01:52<00:06, 106MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  93% 8.63G/9.24G [01:52<00:04, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  94% 8.70G/9.24G [01:52<00:03, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  95% 8.77G/9.24G [01:52<00:02, 178MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  96% 8.84G/9.24G [01:52<00:02, 194MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  96% 8.90G/9.24G [01:53<00:01, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  97% 8.97G/9.24G [01:54<00:02, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  98% 9.04G/9.24G [01:54<00:01, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  99% 9.10G/9.24G [01:55<00:00, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data:  99% 9.17G/9.24G [01:55<00:00, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model.onnx.data: 100% 9.24G/9.24G [01:57<00:00, 78.6MB/s]\n",
            "Download complete. Moving file to /content/3/model.onnx.data\n",
            "Fetching 10 files: 100% 10/10 [01:57<00:00, 11.77s/it]\n",
            "/content/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -m /content/3 -e cuda\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pklXctZI_9KV",
        "outputId": "a172537c-12e4-4fae-b9f1-7cfb815435b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-qa.py\", line 235, in <module>\n",
            "    main(args)\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-qa.py\", line 90, in main\n",
            "    model = og.Model(config)\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Load model from /content/3/ failed:Protobuf parsing failed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/4\n",
        "!huggingface-cli download Prince-1/Gemma-7b-Onnx --local-dir /content/4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-xVBKLrCCLN",
        "outputId": "bbb3ed4c-f31b-4871-e625-52bcff813006"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/4\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Fetching 9 files:   0% 0/9 [00:00<?, ?it/s]Downloading 'tokenizer.model' to '/content/4/.cache/huggingface/download/7iVfz3cUOMr-hyjiqqRDHEwVBAM=.61a7b147390c64585d6c3543dd6fc636906c9af3865a5548f27f31aee1d4c8e2.incomplete'\n",
            "Downloading 'model.onnx.data' to '/content/4/.cache/huggingface/download/Lzxw0O44RMmhTgKWeQQ-MMzHFnE=.e74090b5ba2dcd0f4c7aa8363456d4deb2b92cb7d91ec6186dface6c3907d5f5.incomplete'\n",
            "Downloading 'tokenizer.json' to '/content/4/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.f559f2189f392b4555613965f089e7c4d300b41fbe080bf79da0d676e33ee7f0.incomplete'\n",
            "Downloading 'model.onnx' to '/content/4/.cache/huggingface/download/ihhw_uFzBe-Y54_HOJQmXx4GS8A=.fbe4745107458b763f9a2657a714a74fda29511839a7bb76b7e91299fc22e56a.incomplete'\n",
            "\n",
            "model.onnx.data:   0% 0.00/18.7G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer.model:   0% 0.00/4.24M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/34.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.onnx:   0% 0.00/203k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'README.md' to '/content/4/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.5ff53db793010b458ae243b344129f651f5bdc93.incomplete'\n",
            "Downloading 'special_tokens_map.json' to '/content/4/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.8d6368f7e735fbe4781bf6e956b7c6ad0586df80.incomplete'\n",
            "Downloading 'genai_config.json' to '/content/4/.cache/huggingface/download/yDyKFBRCOr3XTWAm4EHNQUX0s3Y=.d16a03cf6291f99d3ca32aa5ee086af5c64606c0.incomplete'\n",
            "Downloading '.gitattributes' to '/content/4/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.f78df3bfb43291872abf78496110e5856b24de73.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "genai_config.json: 1.51kB [00:00, 6.49MB/s]\n",
            "Download complete. Moving file to /content/4/genai_config.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "special_tokens_map.json: 100% 636/636 [00:00<00:00, 5.64MB/s]\n",
            "Download complete. Moving file to /content/4/special_tokens_map.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 1.62kB [00:00, 10.2MB/s]\n",
            "Download complete. Moving file to /content/4/.gitattributes\n",
            "Fetching 9 files:  11% 1/9 [00:00<00:02,  2.70it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "README.md: 100% 323/323 [00:00<00:00, 3.74MB/s]\n",
            "Download complete. Moving file to /content/4/README.md\n",
            "Downloading 'tokenizer_config.json' to '/content/4/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.25f20a743bbf512a2bd494be5165c84ac577b94f.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 40.0kB [00:00, 13.4MB/s]\n",
            "Download complete. Moving file to /content/4/tokenizer_config.json\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "model.onnx: 100% 203k/203k [00:01<00:00, 173kB/s]\n",
            "Download complete. Moving file to /content/4/model.onnx\n",
            "Fetching 9 files:  44% 4/9 [00:01<00:01,  2.88it/s]\n",
            "\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:01<00:00, 2.92MB/s]\n",
            "Download complete. Moving file to /content/4/tokenizer.model\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 34.4M/34.4M [00:01<00:00, 21.3MB/s]\n",
            "Download complete. Moving file to /content/4/tokenizer.json\n",
            "\n",
            "model.onnx.data:   0% 41.9M/18.7G [00:03<24:53, 12.5MB/s]\u001b[A\n",
            "model.onnx.data:   0% 50.9M/18.7G [00:03<21:12, 14.6MB/s]\u001b[A\n",
            "model.onnx.data:   1% 97.7M/18.7G [00:03<08:37, 35.8MB/s]\u001b[A\n",
            "model.onnx.data:   1% 111M/18.7G [00:04<08:29, 36.4MB/s] \u001b[A\n",
            "model.onnx.data:   1% 125M/18.7G [00:04<09:36, 32.1MB/s]\u001b[A\n",
            "model.onnx.data:   1% 192M/18.7G [00:05<05:54, 52.1MB/s]\u001b[A\n",
            "model.onnx.data:   2% 347M/18.7G [00:05<02:24, 127MB/s] \u001b[A\n",
            "model.onnx.data:   2% 372M/18.7G [00:06<02:24, 127MB/s]\u001b[A\n",
            "model.onnx.data:   2% 439M/18.7G [00:06<02:02, 148MB/s]\u001b[A\n",
            "model.onnx.data:   3% 485M/18.7G [00:11<09:23, 32.2MB/s]\u001b[A\n",
            "model.onnx.data:   3% 552M/18.7G [00:11<06:33, 46.0MB/s]\u001b[A\n",
            "model.onnx.data:   3% 619M/18.7G [00:12<05:18, 56.7MB/s]\u001b[A\n",
            "model.onnx.data:   4% 686M/18.7G [00:12<04:31, 66.2MB/s]\u001b[A\n",
            "model.onnx.data:   4% 753M/18.7G [00:12<03:16, 91.2MB/s]\u001b[A\n",
            "model.onnx.data:   4% 820M/18.7G [00:13<02:40, 111MB/s] \u001b[A\n",
            "model.onnx.data:   5% 887M/18.7G [00:13<02:06, 141MB/s]\u001b[A\n",
            "model.onnx.data:   5% 954M/18.7G [00:13<01:48, 163MB/s]\u001b[A\n",
            "model.onnx.data:   5% 954M/18.7G [00:29<01:48, 163MB/s]\u001b[A\n",
            "model.onnx.data:   5% 1.02G/18.7G [00:34<28:19, 10.4MB/s]\u001b[A\n",
            "model.onnx.data:   6% 1.09G/18.7G [00:34<19:53, 14.7MB/s]\u001b[A\n",
            "model.onnx.data:   6% 1.16G/18.7G [00:34<14:12, 20.5MB/s]\u001b[A\n",
            "model.onnx.data:   7% 1.22G/18.7G [00:34<10:18, 28.2MB/s]\u001b[A\n",
            "model.onnx.data:   7% 1.29G/18.7G [00:35<07:32, 38.3MB/s]\u001b[A\n",
            "model.onnx.data:   7% 1.36G/18.7G [00:35<05:25, 53.2MB/s]\u001b[A\n",
            "model.onnx.data:   8% 1.42G/18.7G [00:35<04:28, 64.2MB/s]\u001b[A\n",
            "model.onnx.data:   8% 1.56G/18.7G [00:36<03:18, 86.0MB/s]\u001b[A\n",
            "model.onnx.data:   9% 1.64G/18.7G [00:36<02:29, 114MB/s] \u001b[A\n",
            "model.onnx.data:   9% 1.71G/18.7G [00:37<02:09, 131MB/s]\u001b[A\n",
            "model.onnx.data:   9% 1.76G/18.7G [00:41<07:20, 38.3MB/s]\u001b[A\n",
            "model.onnx.data:  10% 1.83G/18.7G [00:42<05:31, 50.7MB/s]\u001b[A\n",
            "model.onnx.data:  10% 1.90G/18.7G [00:42<04:23, 63.5MB/s]\u001b[A\n",
            "model.onnx.data:  11% 1.96G/18.7G [00:42<03:15, 85.4MB/s]\u001b[A\n",
            "model.onnx.data:  11% 2.03G/18.7G [00:49<10:07, 27.4MB/s]\u001b[A\n",
            "model.onnx.data:  11% 2.03G/18.7G [00:59<10:07, 27.4MB/s]\u001b[A\n",
            "model.onnx.data:  11% 2.10G/18.7G [01:08<30:48, 8.96MB/s]\u001b[A\n",
            "model.onnx.data:  12% 2.16G/18.7G [01:08<21:51, 12.6MB/s]\u001b[A\n",
            "model.onnx.data:  12% 2.23G/18.7G [01:09<15:38, 17.5MB/s]\u001b[A\n",
            "model.onnx.data:  12% 2.30G/18.7G [01:10<12:19, 22.1MB/s]\u001b[A\n",
            "model.onnx.data:  13% 2.36G/18.7G [01:14<13:28, 20.1MB/s]\u001b[A\n",
            "model.onnx.data:  13% 2.43G/18.7G [01:14<09:42, 27.9MB/s]\u001b[A\n",
            "model.onnx.data:  13% 2.50G/18.7G [01:14<07:07, 37.8MB/s]\u001b[A\n",
            "model.onnx.data:  14% 2.57G/18.7G [01:15<05:12, 51.5MB/s]\u001b[A\n",
            "model.onnx.data:  14% 2.63G/18.7G [01:15<04:23, 60.7MB/s]\u001b[A\n",
            "model.onnx.data:  14% 2.70G/18.7G [01:15<03:15, 81.6MB/s]\u001b[A\n",
            "model.onnx.data:  15% 2.77G/18.7G [01:16<02:39, 99.9MB/s]\u001b[A\n",
            "model.onnx.data:  15% 2.83G/18.7G [01:16<02:17, 115MB/s] \u001b[A\n",
            "model.onnx.data:  16% 2.97G/18.7G [01:17<01:50, 141MB/s]\u001b[A\n",
            "model.onnx.data:  16% 3.04G/18.7G [01:17<01:34, 165MB/s]\u001b[A\n",
            "model.onnx.data:  17% 3.12G/18.7G [01:18<01:42, 151MB/s]\u001b[A\n",
            "model.onnx.data:  17% 3.19G/18.7G [01:18<01:24, 183MB/s]\u001b[A\n",
            "model.onnx.data:  17% 3.25G/18.7G [01:18<01:12, 212MB/s]\u001b[A\n",
            "model.onnx.data:  18% 3.32G/18.7G [01:18<01:14, 206MB/s]\u001b[A\n",
            "model.onnx.data:  18% 3.39G/18.7G [01:19<01:06, 231MB/s]\u001b[A\n",
            "model.onnx.data:  19% 3.45G/18.7G [01:19<00:57, 263MB/s]\u001b[A\n",
            "model.onnx.data:  19% 3.52G/18.7G [01:19<00:57, 263MB/s]\u001b[A\n",
            "model.onnx.data:  19% 3.59G/18.7G [01:19<00:58, 258MB/s]\u001b[A\n",
            "model.onnx.data:  20% 3.66G/18.7G [01:20<01:09, 215MB/s]\u001b[A\n",
            "model.onnx.data:  20% 3.71G/18.7G [01:20<01:02, 238MB/s]\u001b[A\n",
            "model.onnx.data:  20% 3.78G/18.7G [01:20<01:09, 215MB/s]\u001b[A\n",
            "model.onnx.data:  21% 3.84G/18.7G [01:20<01:01, 239MB/s]\u001b[A\n",
            "model.onnx.data:  21% 3.91G/18.7G [01:21<00:58, 251MB/s]\u001b[A\n",
            "model.onnx.data:  21% 3.98G/18.7G [01:21<01:15, 193MB/s]\u001b[A\n",
            "model.onnx.data:  22% 4.03G/18.7G [01:26<07:10, 34.0MB/s]\u001b[A\n",
            "model.onnx.data:  22% 4.16G/18.7G [01:27<04:13, 57.1MB/s]\u001b[A\n",
            "model.onnx.data:  23% 4.23G/18.7G [01:33<08:34, 28.1MB/s]\u001b[A\n",
            "model.onnx.data:  23% 4.30G/18.7G [01:48<20:39, 11.6MB/s]\u001b[A\n",
            "model.onnx.data:  23% 4.36G/18.7G [01:50<16:24, 14.5MB/s]\u001b[A\n",
            "model.onnx.data:  26% 4.77G/18.7G [01:55<06:56, 33.3MB/s]\u001b[A\n",
            "model.onnx.data:  26% 4.83G/18.7G [01:55<05:58, 38.5MB/s]\u001b[A\n",
            "model.onnx.data:  26% 4.90G/18.7G [02:01<08:01, 28.5MB/s]\u001b[A\n",
            "model.onnx.data:  27% 4.97G/18.7G [02:01<06:29, 35.1MB/s]\u001b[A\n",
            "model.onnx.data:  27% 5.03G/18.7G [02:01<05:19, 42.6MB/s]\u001b[A\n",
            "model.onnx.data:  27% 5.10G/18.7G [02:01<04:14, 53.3MB/s]\u001b[A\n",
            "model.onnx.data:  28% 5.17G/18.7G [02:02<03:23, 66.1MB/s]\u001b[A\n",
            "model.onnx.data:  28% 5.24G/18.7G [02:02<02:42, 82.8MB/s]\u001b[A\n",
            "model.onnx.data:  28% 5.30G/18.7G [02:02<02:11, 102MB/s] \u001b[A\n",
            "model.onnx.data:  29% 5.37G/18.7G [02:03<02:11, 101MB/s]\u001b[A\n",
            "model.onnx.data:  29% 5.44G/18.7G [02:12<10:14, 21.5MB/s]\u001b[A\n",
            "model.onnx.data:  29% 5.44G/18.7G [02:29<10:14, 21.5MB/s]\u001b[A\n",
            "model.onnx.data:  30% 5.50G/18.7G [02:50<42:59, 5.10MB/s]\u001b[A\n",
            "model.onnx.data:  30% 5.57G/18.7G [02:50<30:59, 7.03MB/s]\u001b[A\n",
            "model.onnx.data:  31% 5.71G/18.7G [02:51<16:53, 12.8MB/s]\u001b[A\n",
            "model.onnx.data:  31% 5.77G/18.7G [02:51<12:52, 16.7MB/s]\u001b[A\n",
            "model.onnx.data:  31% 5.84G/18.7G [02:51<09:43, 22.0MB/s]\u001b[A\n",
            "model.onnx.data:  32% 5.91G/18.7G [02:52<07:18, 29.1MB/s]\u001b[A\n",
            "model.onnx.data:  32% 5.97G/18.7G [02:52<05:34, 37.9MB/s]\u001b[A\n",
            "model.onnx.data:  32% 6.04G/18.7G [02:53<05:01, 41.9MB/s]\u001b[A\n",
            "model.onnx.data:  33% 6.11G/18.7G [02:59<08:34, 24.4MB/s]\u001b[A\n",
            "model.onnx.data:  33% 6.17G/18.7G [02:59<06:08, 33.9MB/s]\u001b[A\n",
            "model.onnx.data:  33% 6.24G/18.7G [02:59<04:35, 45.0MB/s]\u001b[A\n",
            "model.onnx.data:  34% 6.31G/18.7G [02:59<03:26, 59.7MB/s]\u001b[A\n",
            "model.onnx.data:  34% 6.38G/18.7G [03:00<02:39, 76.9MB/s]\u001b[A\n",
            "model.onnx.data:  35% 6.44G/18.7G [03:00<02:24, 84.5MB/s]\u001b[A\n",
            "model.onnx.data:  35% 6.51G/18.7G [03:01<01:58, 102MB/s] \u001b[A\n",
            "model.onnx.data:  35% 6.58G/18.7G [03:01<01:35, 126MB/s]\u001b[A\n",
            "model.onnx.data:  36% 6.64G/18.7G [03:01<01:17, 155MB/s]\u001b[A\n",
            "model.onnx.data:  36% 6.71G/18.7G [03:01<01:15, 159MB/s]\u001b[A\n",
            "model.onnx.data:  36% 6.78G/18.7G [03:02<01:09, 171MB/s]\u001b[A\n",
            "model.onnx.data:  37% 6.85G/18.7G [03:02<01:04, 184MB/s]\u001b[A\n",
            "model.onnx.data:  37% 6.91G/18.7G [03:02<01:03, 185MB/s]\u001b[A\n",
            "model.onnx.data:  37% 6.98G/18.7G [03:03<00:51, 226MB/s]\u001b[A\n",
            "model.onnx.data:  38% 7.05G/18.7G [03:03<00:43, 269MB/s]\u001b[A\n",
            "model.onnx.data:  38% 7.11G/18.7G [03:03<00:44, 257MB/s]\u001b[A\n",
            "model.onnx.data:  38% 7.18G/18.7G [03:03<00:43, 262MB/s]\u001b[A\n",
            "model.onnx.data:  39% 7.25G/18.7G [03:03<00:44, 258MB/s]\u001b[A\n",
            "model.onnx.data:  39% 7.31G/18.7G [03:04<01:21, 139MB/s]\u001b[A\n",
            "model.onnx.data:  40% 7.40G/18.7G [03:07<02:44, 68.3MB/s]\u001b[A\n",
            "model.onnx.data:  40% 7.47G/18.7G [03:07<02:10, 85.8MB/s]\u001b[A\n",
            "model.onnx.data:  40% 7.54G/18.7G [03:08<01:46, 105MB/s] \u001b[A\n",
            "model.onnx.data:  41% 7.60G/18.7G [03:08<01:22, 134MB/s]\u001b[A\n",
            "model.onnx.data:  41% 7.67G/18.7G [03:08<01:12, 151MB/s]\u001b[A\n",
            "model.onnx.data:  41% 7.74G/18.7G [03:09<01:41, 108MB/s]\u001b[A\n",
            "model.onnx.data:  42% 7.81G/18.7G [03:09<01:21, 132MB/s]\u001b[A\n",
            "model.onnx.data:  42% 7.87G/18.7G [03:15<05:12, 34.5MB/s]\u001b[A\n",
            "model.onnx.data:  43% 7.94G/18.7G [03:15<04:01, 44.3MB/s]\u001b[A\n",
            "model.onnx.data:  43% 8.01G/18.7G [03:15<02:56, 60.4MB/s]\u001b[A\n",
            "model.onnx.data:  43% 8.07G/18.7G [03:16<02:21, 75.0MB/s]\u001b[A\n",
            "model.onnx.data:  44% 8.14G/18.7G [03:21<06:04, 28.8MB/s]\u001b[A\n",
            "model.onnx.data:  44% 8.27G/18.7G [03:22<03:25, 50.4MB/s]\u001b[A\n",
            "model.onnx.data:  45% 8.34G/18.7G [03:22<02:44, 62.9MB/s]\u001b[A\n",
            "model.onnx.data:  45% 8.41G/18.7G [03:22<02:11, 78.1MB/s]\u001b[A\n",
            "model.onnx.data:  45% 8.48G/18.7G [03:22<01:45, 96.8MB/s]\u001b[A\n",
            "model.onnx.data:  46% 8.54G/18.7G [03:23<01:26, 117MB/s] \u001b[A\n",
            "model.onnx.data:  46% 8.61G/18.7G [03:23<01:12, 138MB/s]\u001b[A\n",
            "model.onnx.data:  47% 8.68G/18.7G [03:23<01:02, 158MB/s]\u001b[A\n",
            "model.onnx.data:  47% 8.74G/18.7G [03:23<00:55, 178MB/s]\u001b[A\n",
            "model.onnx.data:  47% 8.81G/18.7G [03:24<00:50, 195MB/s]\u001b[A\n",
            "model.onnx.data:  48% 8.88G/18.7G [03:24<00:47, 204MB/s]\u001b[A\n",
            "model.onnx.data:  48% 8.95G/18.7G [03:25<01:10, 138MB/s]\u001b[A\n",
            "model.onnx.data:  48% 9.01G/18.7G [03:27<02:17, 69.9MB/s]\u001b[A\n",
            "model.onnx.data:  49% 9.08G/18.7G [03:28<02:02, 78.1MB/s]\u001b[A\n",
            "model.onnx.data:  49% 9.15G/18.7G [03:28<01:32, 103MB/s] \u001b[A\n",
            "model.onnx.data:  49% 9.21G/18.7G [03:28<01:31, 103MB/s]\u001b[A\n",
            "model.onnx.data:  50% 9.28G/18.7G [03:29<01:13, 127MB/s]\u001b[A\n",
            "model.onnx.data:  50% 9.35G/18.7G [03:29<01:01, 151MB/s]\u001b[A\n",
            "model.onnx.data:  50% 9.41G/18.7G [03:30<01:12, 128MB/s]\u001b[A\n",
            "model.onnx.data:  51% 9.48G/18.7G [03:36<04:52, 31.3MB/s]\u001b[A\n",
            "model.onnx.data:  52% 9.62G/18.7G [03:36<02:51, 52.8MB/s]\u001b[A\n",
            "model.onnx.data:  52% 9.68G/18.7G [03:42<05:14, 28.5MB/s]\u001b[A\n",
            "model.onnx.data:  53% 9.82G/18.7G [03:42<03:11, 46.2MB/s]\u001b[A\n",
            "model.onnx.data:  53% 9.88G/18.7G [03:42<02:32, 57.3MB/s]\u001b[A\n",
            "model.onnx.data:  53% 9.95G/18.7G [03:43<02:03, 70.7MB/s]\u001b[A\n",
            "model.onnx.data:  54% 10.0G/18.7G [03:43<01:39, 86.6MB/s]\u001b[A\n",
            "model.onnx.data:  54% 10.1G/18.7G [03:48<04:05, 34.9MB/s]\u001b[A\n",
            "model.onnx.data:  55% 10.2G/18.7G [03:49<02:59, 46.9MB/s]\u001b[A\n",
            "model.onnx.data:  57% 10.6G/18.7G [03:50<01:11, 113MB/s] \u001b[A\n",
            "model.onnx.data:  57% 10.6G/18.7G [03:50<01:14, 108MB/s]\u001b[A\n",
            "model.onnx.data:  57% 10.7G/18.7G [03:56<02:56, 45.2MB/s]\u001b[A\n",
            "model.onnx.data:  58% 10.8G/18.7G [03:56<02:22, 55.3MB/s]\u001b[A\n",
            "model.onnx.data:  58% 10.8G/18.7G [03:56<01:58, 66.2MB/s]\u001b[A\n",
            "model.onnx.data:  58% 10.9G/18.7G [03:57<01:36, 80.4MB/s]\u001b[A\n",
            "model.onnx.data:  59% 11.0G/18.7G [03:57<01:19, 96.9MB/s]\u001b[A\n",
            "model.onnx.data:  59% 11.0G/18.7G [03:57<01:04, 117MB/s] \u001b[A\n",
            "model.onnx.data:  59% 11.1G/18.7G [03:58<00:54, 139MB/s]\u001b[A\n",
            "model.onnx.data:  60% 11.2G/18.7G [03:58<00:46, 162MB/s]\u001b[A\n",
            "model.onnx.data:  60% 11.2G/18.7G [03:58<00:48, 153MB/s]\u001b[A\n",
            "model.onnx.data:  61% 11.3G/18.7G [03:59<00:47, 156MB/s]\u001b[A\n",
            "model.onnx.data:  61% 11.4G/18.7G [03:59<00:36, 199MB/s]\u001b[A\n",
            "model.onnx.data:  62% 11.5G/18.7G [03:59<00:36, 197MB/s]\u001b[A\n",
            "model.onnx.data:  62% 11.5G/18.7G [04:00<00:38, 187MB/s]\u001b[A\n",
            "model.onnx.data:  62% 11.6G/18.7G [04:00<00:32, 216MB/s]\u001b[A\n",
            "model.onnx.data:  63% 11.7G/18.7G [04:00<00:28, 249MB/s]\u001b[A\n",
            "model.onnx.data:  63% 11.7G/18.7G [04:00<00:27, 252MB/s]\u001b[A\n",
            "model.onnx.data:  63% 11.8G/18.7G [04:01<00:27, 252MB/s]\u001b[A\n",
            "model.onnx.data:  64% 11.9G/18.7G [04:01<00:25, 262MB/s]\u001b[A\n",
            "model.onnx.data:  64% 11.9G/18.7G [04:01<00:28, 232MB/s]\u001b[A\n",
            "model.onnx.data:  64% 12.0G/18.7G [04:02<00:32, 207MB/s]\u001b[A\n",
            "model.onnx.data:  65% 12.1G/18.7G [04:02<00:28, 232MB/s]\u001b[A\n",
            "model.onnx.data:  65% 12.1G/18.7G [04:02<00:24, 265MB/s]\u001b[A\n",
            "model.onnx.data:  65% 12.2G/18.7G [04:02<00:23, 269MB/s]\u001b[A\n",
            "model.onnx.data:  66% 12.3G/18.7G [04:03<00:23, 271MB/s]\u001b[A\n",
            "model.onnx.data:  66% 12.3G/18.7G [04:06<02:02, 51.4MB/s]\u001b[A\n",
            "model.onnx.data:  67% 12.4G/18.7G [04:07<01:29, 69.7MB/s]\u001b[A\n",
            "model.onnx.data:  67% 12.5G/18.7G [04:07<01:08, 89.5MB/s]\u001b[A\n",
            "model.onnx.data:  67% 12.6G/18.7G [04:07<00:58, 104MB/s] \u001b[A\n",
            "model.onnx.data:  68% 12.6G/18.7G [04:08<00:46, 129MB/s]\u001b[A\n",
            "model.onnx.data:  68% 12.7G/18.7G [04:08<00:35, 166MB/s]\u001b[A\n",
            "model.onnx.data:  68% 12.8G/18.7G [04:08<00:40, 145MB/s]\u001b[A\n",
            "model.onnx.data:  69% 12.8G/18.7G [04:09<00:34, 167MB/s]\u001b[A\n",
            "model.onnx.data:  69% 12.9G/18.7G [04:09<00:26, 213MB/s]\u001b[A\n",
            "model.onnx.data:  70% 13.0G/18.7G [04:09<00:24, 230MB/s]\u001b[A\n",
            "model.onnx.data:  70% 13.0G/18.7G [04:09<00:25, 221MB/s]\u001b[A\n",
            "model.onnx.data:  70% 13.1G/18.7G [04:09<00:21, 256MB/s]\u001b[A\n",
            "model.onnx.data:  71% 13.2G/18.7G [04:10<00:23, 238MB/s]\u001b[A\n",
            "model.onnx.data:  71% 13.2G/18.7G [04:10<00:23, 235MB/s]\u001b[A\n",
            "model.onnx.data:  71% 13.3G/18.7G [04:10<00:21, 250MB/s]\u001b[A\n",
            "model.onnx.data:  72% 13.4G/18.7G [04:11<00:21, 249MB/s]\u001b[A\n",
            "model.onnx.data:  72% 13.4G/18.7G [04:11<00:19, 262MB/s]\u001b[A\n",
            "model.onnx.data:  72% 13.5G/18.7G [04:11<00:21, 240MB/s]\u001b[A\n",
            "model.onnx.data:  73% 13.6G/18.7G [04:11<00:18, 269MB/s]\u001b[A\n",
            "model.onnx.data:  73% 13.6G/18.7G [04:11<00:18, 273MB/s]\u001b[A\n",
            "model.onnx.data:  74% 13.7G/18.7G [04:12<00:19, 254MB/s]\u001b[A\n",
            "model.onnx.data:  74% 13.8G/18.7G [04:12<00:17, 287MB/s]\u001b[A\n",
            "model.onnx.data:  74% 13.8G/18.7G [04:12<00:15, 311MB/s]\u001b[A\n",
            "model.onnx.data:  75% 13.9G/18.7G [04:13<00:19, 241MB/s]\u001b[A\n",
            "model.onnx.data:  75% 14.0G/18.7G [04:14<00:53, 87.1MB/s]\u001b[A\n",
            "model.onnx.data:  75% 14.0G/18.7G [04:15<00:39, 116MB/s] \u001b[A\n",
            "model.onnx.data:  76% 14.1G/18.7G [04:15<00:31, 142MB/s]\u001b[A\n",
            "model.onnx.data:  76% 14.2G/18.7G [04:15<00:29, 153MB/s]\u001b[A\n",
            "model.onnx.data:  76% 14.2G/18.7G [04:16<00:31, 138MB/s]\u001b[A\n",
            "model.onnx.data:  77% 14.3G/18.7G [04:16<00:33, 130MB/s]\u001b[A\n",
            "model.onnx.data:  77% 14.4G/18.7G [04:17<00:29, 144MB/s]\u001b[A\n",
            "model.onnx.data:  77% 14.4G/18.7G [04:17<00:27, 153MB/s]\u001b[A\n",
            "model.onnx.data:  78% 14.5G/18.7G [04:17<00:23, 175MB/s]\u001b[A\n",
            "model.onnx.data:  78% 14.6G/18.7G [04:18<00:24, 165MB/s]\u001b[A\n",
            "model.onnx.data:  79% 14.7G/18.7G [04:18<00:19, 201MB/s]\u001b[A\n",
            "model.onnx.data:  79% 14.8G/18.7G [04:19<00:20, 190MB/s]\u001b[A\n",
            "model.onnx.data:  80% 14.9G/18.7G [04:19<00:23, 161MB/s]\u001b[A\n",
            "model.onnx.data:  80% 14.9G/18.7G [04:23<01:07, 55.6MB/s]\u001b[A\n",
            "model.onnx.data:  81% 15.1G/18.7G [04:23<00:43, 81.8MB/s]\u001b[A\n",
            "model.onnx.data:  81% 15.1G/18.7G [04:23<00:35, 101MB/s] \u001b[A\n",
            "model.onnx.data:  81% 15.2G/18.7G [04:24<00:30, 114MB/s]\u001b[A\n",
            "model.onnx.data:  82% 15.3G/18.7G [04:27<00:59, 57.5MB/s]\u001b[A\n",
            "model.onnx.data:  82% 15.3G/18.7G [04:27<00:44, 74.2MB/s]\u001b[A\n",
            "model.onnx.data:  82% 15.4G/18.7G [04:27<00:34, 95.1MB/s]\u001b[A\n",
            "model.onnx.data:  83% 15.5G/18.7G [04:27<00:30, 107MB/s] \u001b[A\n",
            "model.onnx.data:  83% 15.5G/18.7G [04:28<00:24, 130MB/s]\u001b[A\n",
            "model.onnx.data:  84% 15.6G/18.7G [04:28<00:21, 146MB/s]\u001b[A\n",
            "model.onnx.data:  84% 15.7G/18.7G [04:28<00:17, 167MB/s]\u001b[A\n",
            "model.onnx.data:  84% 15.7G/18.7G [04:28<00:15, 189MB/s]\u001b[A\n",
            "model.onnx.data:  85% 15.8G/18.7G [04:29<00:14, 200MB/s]\u001b[A\n",
            "model.onnx.data:  85% 15.9G/18.7G [04:29<00:12, 222MB/s]\u001b[A\n",
            "model.onnx.data:  85% 15.9G/18.7G [04:29<00:12, 216MB/s]\u001b[A\n",
            "model.onnx.data:  86% 16.0G/18.7G [04:30<00:12, 213MB/s]\u001b[A\n",
            "model.onnx.data:  86% 16.0G/18.7G [04:30<00:10, 243MB/s]\u001b[A\n",
            "model.onnx.data:  86% 16.1G/18.7G [04:30<00:09, 269MB/s]\u001b[A\n",
            "model.onnx.data:  87% 16.2G/18.7G [04:30<00:08, 283MB/s]\u001b[A\n",
            "model.onnx.data:  87% 16.2G/18.7G [04:31<00:11, 214MB/s]\u001b[A\n",
            "model.onnx.data:  87% 16.3G/18.7G [04:31<00:09, 253MB/s]\u001b[A\n",
            "model.onnx.data:  88% 16.4G/18.7G [04:31<00:13, 170MB/s]\u001b[A\n",
            "model.onnx.data:  88% 16.4G/18.7G [04:32<00:10, 210MB/s]\u001b[A\n",
            "model.onnx.data:  88% 16.5G/18.7G [04:32<00:09, 238MB/s]\u001b[A\n",
            "model.onnx.data:  89% 16.6G/18.7G [04:32<00:08, 249MB/s]\u001b[A\n",
            "model.onnx.data:  89% 16.6G/18.7G [04:33<00:10, 184MB/s]\u001b[A\n",
            "model.onnx.data:  90% 16.8G/18.7G [04:37<00:33, 55.7MB/s]\u001b[A\n",
            "model.onnx.data:  91% 16.9G/18.7G [04:37<00:19, 87.2MB/s]\u001b[A\n",
            "model.onnx.data:  91% 17.0G/18.7G [04:38<00:16, 98.7MB/s]\u001b[A\n",
            "model.onnx.data:  91% 17.0G/18.7G [04:38<00:13, 118MB/s] \u001b[A\n",
            "model.onnx.data:  92% 17.1G/18.7G [04:38<00:11, 131MB/s]\u001b[A\n",
            "model.onnx.data:  92% 17.2G/18.7G [04:38<00:09, 156MB/s]\u001b[A\n",
            "model.onnx.data:  92% 17.2G/18.7G [04:39<00:08, 174MB/s]\u001b[A\n",
            "model.onnx.data:  93% 17.3G/18.7G [04:39<00:06, 195MB/s]\u001b[A\n",
            "model.onnx.data:  93% 17.4G/18.7G [04:41<00:16, 75.1MB/s]\u001b[A\n",
            "model.onnx.data:  94% 17.4G/18.7G [04:41<00:12, 96.5MB/s]\u001b[A\n",
            "model.onnx.data:  94% 17.5G/18.7G [04:42<00:09, 124MB/s] \u001b[A\n",
            "model.onnx.data:  94% 17.6G/18.7G [04:42<00:07, 148MB/s]\u001b[A\n",
            "model.onnx.data:  95% 17.6G/18.7G [04:42<00:06, 164MB/s]\u001b[A\n",
            "model.onnx.data:  95% 17.7G/18.7G [04:42<00:04, 194MB/s]\u001b[A\n",
            "model.onnx.data:  95% 17.8G/18.7G [04:43<00:04, 210MB/s]\u001b[A\n",
            "model.onnx.data:  96% 17.8G/18.7G [04:43<00:03, 250MB/s]\u001b[A\n",
            "model.onnx.data:  96% 17.9G/18.7G [04:43<00:04, 173MB/s]\u001b[A\n",
            "model.onnx.data:  96% 18.0G/18.7G [04:43<00:03, 211MB/s]\u001b[A\n",
            "model.onnx.data:  97% 18.0G/18.7G [04:44<00:02, 236MB/s]\u001b[A\n",
            "model.onnx.data:  97% 18.1G/18.7G [04:44<00:02, 252MB/s]\u001b[A\n",
            "model.onnx.data:  97% 18.2G/18.7G [04:44<00:01, 257MB/s]\u001b[A\n",
            "model.onnx.data:  98% 18.2G/18.7G [04:44<00:01, 268MB/s]\u001b[A\n",
            "model.onnx.data:  98% 18.3G/18.7G [04:45<00:01, 267MB/s]\u001b[A\n",
            "model.onnx.data:  99% 18.4G/18.7G [04:45<00:01, 261MB/s]\u001b[A\n",
            "model.onnx.data:  99% 18.5G/18.7G [04:45<00:00, 257MB/s]\u001b[A\n",
            "model.onnx.data:  99% 18.5G/18.7G [04:45<00:00, 245MB/s]\u001b[A\n",
            "model.onnx.data: 100% 18.6G/18.7G [04:46<00:00, 285MB/s]\u001b[A\n",
            "model.onnx.data: 100% 18.7G/18.7G [04:46<00:00, 65.1MB/s]\n",
            "Download complete. Moving file to /content/4/model.onnx.data\n",
            "Fetching 9 files: 100% 9/9 [04:46<00:00, 31.86s/it]\n",
            "/content/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -m /content/4 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HSkuxdHDOg2",
        "outputId": "7d1b6b6d-e60a-4157-a3ff-d4c38aeb27b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "\u001b[1;31m2025-11-18 22:26:17.805639745 [E:onnxruntime:onnxruntime-genai, inference_session.cc:2544 operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 150994944\n",
            "\u001b[m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-qa.py\", line 235, in <module>\n",
            "    main(args)\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-qa.py\", line 90, in main\n",
            "    model = og.Model(config)\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 150994944\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -m /content/4 -e cuda, cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqpO_ggIEwcY",
        "outputId": "337b96eb-d210-4ce7-8a0a-adca4c951b13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "usage: model-qa.py [-h] -m MODEL_PATH\n",
            "                   [-e {cpu,cuda,dml,NvTensorRtRtx,follow_config}]\n",
            "                   [-i MIN_LENGTH] [-l MAX_LENGTH] [-ds] [-p TOP_P] [-k TOP_K]\n",
            "                   [-t TEMPERATURE] [-re REPETITION_PENALTY] [-v] [-g]\n",
            "                   [-gtype {none,json_schema,regex,lark_grammar}]\n",
            "                   [-ginfo GUIDANCE_INFO] [-s SYSTEM_PROMPT]\n",
            "                   [-inp INPUT_PROMPT]\n",
            "model-qa.py: error: argument -e/--execution_provider: invalid choice: 'cuda,' (choose from cpu, cuda, dml, NvTensorRtRtx, follow_config)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -m /content/4 -e follow_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P0H49FwE67v",
        "outputId": "508d466c-9154-4a9a-d57b-ad2bd6bb771a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/onnxruntime-genai-0.11.2/examples/python\n",
        "!python model-qa.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aou4l7puFBh3",
        "outputId": "6989e418-ccdc-40bb-fb63-c74fa43e23a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/onnxruntime-genai-0.11.2/examples/python\n",
            "usage: model-qa.py [-h] -m MODEL_PATH\n",
            "                   [-e {cpu,cuda,dml,NvTensorRtRtx,follow_config}]\n",
            "                   [-i MIN_LENGTH] [-l MAX_LENGTH] [-ds] [-p TOP_P] [-k TOP_K]\n",
            "                   [-t TEMPERATURE] [-re REPETITION_PENALTY] [-v] [-g]\n",
            "                   [-gtype {none,json_schema,regex,lark_grammar}]\n",
            "                   [-ginfo GUIDANCE_INFO] [-s SYSTEM_PROMPT]\n",
            "                   [-inp INPUT_PROMPT]\n",
            "\n",
            "End-to-end AI Question/Answer example for gen-ai\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -m MODEL_PATH, --model_path MODEL_PATH\n",
            "                        Onnx model folder path (must contain genai_config.json\n",
            "                        and model.onnx)\n",
            "  -e {cpu,cuda,dml,NvTensorRtRtx,follow_config}, --execution_provider {cpu,cuda,dml,NvTensorRtRtx,follow_config}\n",
            "                        Execution provider to run the ONNX Runtime session\n",
            "                        with. Defaults to follow_config that uses the\n",
            "                        execution provider listed in the genai_config.json\n",
            "                        instead.\n",
            "  -i MIN_LENGTH, --min_length MIN_LENGTH\n",
            "                        Min number of tokens to generate including the prompt\n",
            "  -l MAX_LENGTH, --max_length MAX_LENGTH\n",
            "                        Max number of tokens to generate including the prompt\n",
            "  -ds, --do_sample      Do random sampling. When false, greedy or beam search\n",
            "                        are used to generate the output. Defaults to false\n",
            "  -p TOP_P, --top_p TOP_P\n",
            "                        Top p probability to sample with\n",
            "  -k TOP_K, --top_k TOP_K\n",
            "                        Top k tokens to sample from\n",
            "  -t TEMPERATURE, --temperature TEMPERATURE\n",
            "                        Temperature to sample with\n",
            "  -re REPETITION_PENALTY, --repetition_penalty REPETITION_PENALTY\n",
            "                        Repetition penalty to sample with\n",
            "  -v, --verbose         Print verbose output and timing information. Defaults\n",
            "                        to false\n",
            "  -g, --timings         Print timing information for each generation step.\n",
            "                        Defaults to false\n",
            "  -gtype {none,json_schema,regex,lark_grammar}, --guidance_type {none,json_schema,regex,lark_grammar}\n",
            "                        Provide guidance type for the model, options are\n",
            "                        json_schema, regex, or lark_grammar.\n",
            "  -ginfo GUIDANCE_INFO, --guidance_info GUIDANCE_INFO\n",
            "                        Provide information of the guidance type used, it\n",
            "                        could be either tools or regex string. It is required\n",
            "                        if guidance_type is provided\n",
            "  -s SYSTEM_PROMPT, --system_prompt SYSTEM_PROMPT\n",
            "                        System prompt to use for the prompt.\n",
            "  -inp INPUT_PROMPT, --input_prompt INPUT_PROMPT\n",
            "                        Input Prompt, if provided it will just run the prompt\n",
            "                        and exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m /content/4 -e cuda,cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO67BD8hFSE2",
        "outputId": "f3a0b5a2-1bf2-46bf-ccc2-2a0132eb0276"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: model-qa.py [-h] -m MODEL_PATH\n",
            "                   [-e {cpu,cuda,dml,NvTensorRtRtx,follow_config}]\n",
            "                   [-i MIN_LENGTH] [-l MAX_LENGTH] [-ds] [-p TOP_P] [-k TOP_K]\n",
            "                   [-t TEMPERATURE] [-re REPETITION_PENALTY] [-v] [-g]\n",
            "                   [-gtype {none,json_schema,regex,lark_grammar}]\n",
            "                   [-ginfo GUIDANCE_INFO] [-s SYSTEM_PROMPT]\n",
            "                   [-inp INPUT_PROMPT]\n",
            "model-qa.py: error: argument -e/--execution_provider: invalid choice: 'cuda,cpu' (choose from cpu, cuda, dml, NvTensorRtRtx, follow_config)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[-h] -m MODEL_PATH\n",
        "                   [-e {cpu,cuda,dml,NvTensorRtRtx,follow_config}]\n",
        "                   [-i MIN_LENGTH] [-l MAX_LENGTH] [-ds] [-p TOP_P] [-k TOP_K]\n",
        "                   [-t TEMPERATURE] [-re REPETITION_PENALTY] [-v] [-g]\n",
        "                   [-gtype {none,json_schema,regex,lark_grammar}]\n",
        "                   [-ginfo GUIDANCE_INFO] [-s SYSTEM_PROMPT]\n",
        "                   [-inp INPUT_PROMPT]\n",
        "model-qa.py: error: argument -e/--execution_provider: invalid choice: 'cuda,cpu' (choose from cpu, cuda, dml, NvTensorRtRtx, follow_config)\n",
        "\n",
        "[ ]\n",
        "1\n"
      ],
      "metadata": {
        "id": "yL6tPuLxFwPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py"
      ],
      "metadata": {
        "id": "Xhezm6S0Hf83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-generate.py -m /content/4 -e cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA_qRAJsHeJu",
        "outputId": "3f30f1d1-1b4d-470d-c12f-1df79a00112f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: how are you?\n",
            "\u001b[1;31m2025-11-18 22:40:47.834774780 [E:onnxruntime:onnxruntime-genai, inference_session.cc:2544 operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 1572864000\n",
            "\u001b[m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py\", line 122, in <module>\n",
            "    main(args)\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py\", line 52, in main\n",
            "    model = og.Model(config)\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 1572864000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-generate.py -m /content/4 -e cuda -l 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJA8Y7o_Him-",
        "outputId": "16e6747e-1f98-4008-d151-dbd63400f54a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: hi\n",
            "\u001b[1;31m2025-11-18 22:42:05.589895736 [E:onnxruntime:onnxruntime-genai, inference_session.cc:2544 operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 1572864000\n",
            "\u001b[m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py\", line 122, in <module>\n",
            "    main(args)\n",
            "  File \"/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py\", line 52, in main\n",
            "    model = og.Model(config)\n",
            "            ^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Exception during initialization: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:359 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*) Failed to allocate memory for requested buffer of size 1572864000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wbHx_KrIUaA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}