# -*- coding: utf-8 -*-
"""suc_large_onnx_cuda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n8KdpgFwDgIaUmNouOcWW9jWYc8BBpU3
"""

!pip install onnxruntime-genai-cuda

!huggingface-cli download Prince-1/Granite-3.3-2B-Instruct-Onnx --local-dir .

!git clone https://github.com/microsoft/onnxruntime-genai.git

!wget https://github.com/microsoft/onnxruntime-genai/archive/refs/tags/v0.11.2.zip

!unzip v0.11.2.zip



# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai

!git checkout rel-0.11.2

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-chat.py -m /content/1 -e cuda

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-generate.py -m /content/1 -e cuda

/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-generate.py -h

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-generate.py -m /content/1 -e cuda -pr hi -l 22

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-generate.py -m /content/1 -e cuda -pr "what is ai?" -l 22

!python model-generate.py -m /content/1 -e cuda -c '<|user|>\n{input} <|end|>\n<|assistant|>'

!python model-generate.py -m /content/1 -e cuda -c '<|user|>{input}<|end|><|assistant|>'

import onnxruntime_genai as og
import sys

# **1. تحديد المسار ومزود التنفيذ (CUDA)**
model_path = '/content/1'  # يجب أن يتطابق مع المسار /content/1 الذي استخدمته
chat_template = '<|user|>\n{input} <|end|>\n<|assistant|>'

config = og.Config(model_path)
config.set_execution_provider("cuda") # الحل لمشكلة 'Unknown provider name'

try:
    model = og.Model(config)
except RuntimeError as e:
    print(f"Failed to load model: {e}")
    sys.exit()

tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

# 2. إعدادات Generator
search_options = {}
search_options['max_length'] = 2048
search_options['batch_size'] = 1

# 3. حلقة الدردشة
while True:
    text = input("Input: ")
    if text.lower() in ['quit', 'exit', 'quit()', 'exit()']:
        break
    if not text:
        print("Error, input cannot be empty")
        continue

    # تطبيق قالب الدردشة
    prompt = f'{chat_template.format(input=text)}'

    input_tokens = tokenizer.encode(prompt)

    params = og.GeneratorParams(model)
    params.set_search_options(**search_options)

    # لضمان عدم إعادة استخدام Generator في حلقة، قم بإنشائه داخل الحلقة
    generator = og.Generator(model, params)

    print("Output: ", end='', flush=True)

    try:
        generator.append_tokens(input_tokens)
        while True:
            generator.generate_next_token()
            if generator.is_done():
                break
            new_token = generator.get_next_tokens()[0]
            print(tokenizer_stream.decode(new_token), end='', flush=True)
    except KeyboardInterrupt:
        print("  --control+c pressed, aborting generation--")
    except Exception as e:
        print(f"\nGeneration Error: {e}")

    print()
    del generator # تحرير ذاكرة Generator

!python model-chat.py -m /content/1 -e cuda

/content/onnxruntime-genai-0.11.2/examples/python/model-qa.py



!python model-qa.py -m /content/1 -e cuda

/content/onnxruntime-genai-0.11.2/examples/python/guidance-example.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -m /content/1 -e cuda





# Commented out IPython magic to ensure Python compatibility.
# %cd /content/2
!huggingface-cli download Prince-1/TinyLlama-1.1B-Chat-v1.0-Onnx --local-dir /content/2

# Commented out IPython magic to ensure Python compatibility.


# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -m /content/2 -e cuda

# Commented out IPython magic to ensure Python compatibility.


# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -m /content/1 -e cuda

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/3
!huggingface-cli download Prince-1/Medgemma-4b-pt-Onnx --local-dir /content/3

# Commented out IPython magic to ensure Python compatibility.


# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -m /content/3 -e cuda

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/4
!huggingface-cli download Prince-1/Gemma-7b-Onnx --local-dir /content/4

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -m /content/4 -e cuda

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -m /content/4 -e cuda, cpu

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -m /content/4 -e follow_config

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/onnxruntime-genai-0.11.2/examples/python
!python model-qa.py -h

!python model-qa.py -m /content/4 -e cuda,cpu

[-h] -m MODEL_PATH
                   [-e {cpu,cuda,dml,NvTensorRtRtx,follow_config}]
                   [-i MIN_LENGTH] [-l MAX_LENGTH] [-ds] [-p TOP_P] [-k TOP_K]
                   [-t TEMPERATURE] [-re REPETITION_PENALTY] [-v] [-g]
                   [-gtype {none,json_schema,regex,lark_grammar}]
                   [-ginfo GUIDANCE_INFO] [-s SYSTEM_PROMPT]
                   [-inp INPUT_PROMPT]
model-qa.py: error: argument -e/--execution_provider: invalid choice: 'cuda,cpu' (choose from cpu, cuda, dml, NvTensorRtRtx, follow_config)

[ ]
1

/content/onnxruntime-genai-0.11.2/examples/python/model-generate.py

!python model-generate.py -m /content/4 -e cuda

!python model-generate.py -m /content/4 -e cuda -l 5

